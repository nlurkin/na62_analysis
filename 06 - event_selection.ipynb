{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed54cb3-33bf-474a-a3f9-64694bf2f1e9",
   "metadata": {},
   "source": [
    "# Event selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cb673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets first import all we need\n",
    "import uproot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from functools import partial\n",
    "from na62 import prepare, hlf, extract, constants, stats, histo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540c36bd-f340-4f6d-a715-42d589363496",
   "metadata": {},
   "source": [
    "## Data processing: from raw data to high-level objects\n",
    "The NA62 data, after being acquired, are first reconstructed. This means we take our raw (binary) data and we extract for each event the signals recorded for each detector individually. For each detectors, these signals are reconstructed as hits containing information like time, position, and geometrical channel ID (i.e. related to physical position rather than electronics position). Hits are further grouped together to reconstruct and form candidates: candidates are more complex objects generally meant to relate to a single particle interacting with the detector. These candidates can be referred to with other names for certain detectors: LKr -> cluster, Spectrometer -> track, GTK -> beam track.\n",
    "\n",
    "At the analysis level, candidates from multiple detectors can be associated together in space (geometrically) and in time to form higher level objects:\n",
    " - beam kaon: GTK track associated to a KTAG candidate\n",
    " - downstream track: spectrometer track associated to eventual candidates from CHOD, NewCHOD, MUV1-3, LKr, RICH\n",
    " - vertex: association between GTK and spectrometer tracks, or associated between spectrometer tracks only, or association between LKr clusters (neutral vertex)\n",
    "\n",
    "## Pile-up\n",
    "As you may have guessed, the data presented to you here consist of events in the form of association of several of these high-level objects. However to reach that point, there has been already a sizable amount of selection performed at analysis level to make these associations. In fact one single event will usually consist in many more of those high-level objects coming from pile-up. Pile-up refers to valid events (either a beam kaon decay, a beam pion decay, a single beam muon) which are *not* (with caveat) related to the event that generated the trigger. In order to make sure we have **all** the information relating to that specific events, we are requesting signals from detector that are spread within ~100 ns of the trigger time. This means we have a lot of pile-up in each event, which is spread in time randomly within the event. But by applying timing cuts at the analysis level, we can easily discard a very large fraction of those pile-up events. There is still a small fraction of events remaining which happen to be at the same time as the triggered event. As it is not possible to distinguish it from the triggered event, additional selection criterion must be applied to ensure it is a valid kaon decay. As the pile-up event is at the same time as the kaon decay, we usually have several possibilities to combine the high-level objects. For instance, if you have one track and 3 isolated clusters and looking for a K2pi decay, you have three association possibilities for the clusters: combine clusters 1 and 2 to form a pi0, combines cluster 2 and 3, or combines clusters 1 and 3. This is called the combinatorics and you can apply conditions on each of all the possible options. If you manage to find a single option that satisfies all your requirements, this could be the correct one and you go ahead forming an event with this selection of high-level object. If not you usually reject the event.\n",
    "\n",
    "This process of eliminating the pile-up has already been done for you in the data that are available here, and this is the reason why each of your event has only one to three tracks and zero to two clusters. There remains one category of pile-up contaminating your sample (the caveat mentionned above): events that are **not** a kaon decay, but a combination of independent particles that look like a kaon decay. These required additional selection criterion, some of those have already been applied and some that we can apply ourselves in these data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31da7f08",
   "metadata": {},
   "source": [
    "## Pre-selection\n",
    "As mentionned above, a comprehensive pre-selection has already been applied to the data available here. You will find below the details on the criterion that have been used for each of the selected 'event_type'.\n",
    "\n",
    "### K3pi selection\n",
    " - Control trigger\n",
    " - Exactly one three-track vertex within 6 ns of the trigger time\n",
    " - Each track must be in the acceptance of NewCHOD and 4 spectrometer chambers\n",
    " - Tracks must be further than 10 cm away from each other at Straw1\n",
    " - $q_\\text{vtx} = +1$\n",
    " - Vertex between 104 m and 180 m and $\\chi^2<25$\n",
    " - Reconstructed total momentum $p_\\text{tot}$ must be within 3 GeV of 75 GeV\n",
    " - Reconstructed three-pion invariant mass $m_{3\\pi}$ must between 490 MeV and 497 MeV\n",
    "\n",
    "### Kmu2 selection\n",
    " - Control trigger\n",
    " - Less than 10 tracks\n",
    " - Exactly one good track:\n",
    "     - Must have CHOD or NewCHOD associated signal\n",
    "     - Within 10 ns from the trigger time\n",
    "     - Signal in 4 spectrometer chambers\n",
    "     - Track $\\chi^2<20$\n",
    "     - Not forming a vertex with any other track. Vertex if:\n",
    "         - cda < 50 mm\n",
    "         - z vertex between 60 m and 200 m\n",
    " - $q=+1$\n",
    " - Track momentum betwen 5 GeV and 70 GeV\n",
    " - Track in geometric acceptance of all 4 spectrometer stations, MUV3\n",
    " - Vertex between 120 m and 180 m with CDA<40 mm\n",
    " - Closest KTAG candidate in time must be within 2 ns of the track CHOD time, or 5 ns from the track NewCHOD time if CHOD time is not available\n",
    " - No in-time activity in the LAV, IRC and SAC\n",
    " - Track must have MUV3 signal associated to the track within 1.5 ns of the KTAG time, and within 2 ns of the track CHOD time (5 ns of NewCHOD if CHOD not available)\n",
    " - Track $E/p < 0.2$\n",
    " - Missing mass squared (muon hypothesis) $m_\\text{miss}^2(\\mu) < 0.02~\\text{GeV}^2$\n",
    "\n",
    "### K2pi selection\n",
    " - control trigger\n",
    " - At least 1 track and 3 clusters\n",
    " - At least 2 good clusters:\n",
    "     - Within 20 ns of the trigger time\n",
    "     - Further than 20 mm from a dead cell\n",
    "     - Cluster must be electromagnetic\n",
    "     - At least 1 GeV\n",
    "     - Not associated to a Spectrometer track\n",
    "     - Must be isolated\n",
    " - Build pairs of clusters as pi0:\n",
    "   - Within 5 ns of each other\n",
    "   - Further than 200 mm apart\n",
    "   - Energy sum between 2 GeV and 75 GeV\n",
    " - Exactly 1 good track definition:\n",
    "     - At least one pi0 within 2 ns of the track\n",
    "     - $q=+1$\n",
    "     - Not fake\n",
    "     - Less than 20 GeV between raw momentum and fitted momentum\n",
    "     - Momentum between 5 GeV and 70 GeV\n",
    "     - Vertex wrt. beam axis must be between 105 m and 180 m\n",
    "     - CDA < 30 mm\n",
    "     - Inside the geometric acceptance of NewCHOD, all 4 Spectrometer stations, LKr\n",
    " - Select closest pi0 in time\n",
    " - No MUV3 signal associated to the track\n",
    " - Must have LKr cluster associated to the track, and $E/p < 0.9$\n",
    " - No in-time activity in the LAV, IRC and SAC\n",
    " - Reconstructed kaon mass $m_{\\pi\\gamma\\gamma}$ between 460 MeV and 520 MeV\n",
    " - Missing mass squared (pion hypothesis) $m_\\text{miss}^2(\\pi) < 0.015~\\text{GeV}^2$\n",
    "\n",
    "### Kmu3 selection\n",
    " - Control trigger\n",
    " - At least one track\n",
    " - Exactly one good track:\n",
    "     - Must have MUV3 association\n",
    "     - Vertex wrt. beam axis must be between 110 m and 180 m\n",
    "     - CDA < 25 mm\n",
    "     - Momentum between 5 GeV and 50 GeV\n",
    "     - $q = +1$\n",
    "     - Track $\\chi^2 < 20$\n",
    "     - In acceptance of NewCHOD, 4 spectrometer chambers, LKr and MUV3\n",
    " - No other track within 10 ns of the good track\n",
    " - Must have exactly two good LKr clusters:\n",
    "     - Energy > 2 GeV\n",
    "     - Within 6 ns of the track\n",
    "     - Further than 150 mm from the track impact point on LKr\n",
    " - Neutral vertex within 10 m of charged vertex\n",
    " - No in-time activity in the LAV, IRC and SAC\n",
    " - Reconstructed total momentum $p_\\text{tot}$ must be between 15 GeV and 70 GeV\n",
    " - Reconstructed transverse total momentum $p_{\\text{tot},T}$ must be between 40 and 250 MeV\n",
    " - Missing mass squared (muon hypothesis) $m_\\text{miss}^2(\\mu) < 0.01~\\text{GeV}^2$\n",
    "\n",
    "### Ke3 selection\n",
    " - Control trigger\n",
    " - At least one track\n",
    " - Exactly one good track:\n",
    "     - Must not have MUV3 association\n",
    "     - Vertex wrt. beam axis must be between 110 m and 180 m\n",
    "     - CDA < 25 mm\n",
    "     - Momentum between 5 GeV and 50 GeV\n",
    "     - $q = +1$\n",
    "     - Track $\\chi^2 < 20$\n",
    "     - In acceptance of NewCHOD, 4 spectrometer chambers, LKr and MUV3\n",
    " - No other track within 10 ns of the good track\n",
    " - Must have exactly two good LKr clusters:\n",
    "     - Energy > 2 GeV\n",
    "     - Within 6 ns of the track\n",
    "     - Further than 150 mm from the track impact point on LKr\n",
    " - Neutral vertex within 10 m of charged vertex\n",
    " - No in-time activity in the LAV, IRC and SAC\n",
    " - Reconstructed total momentum $p_\\text{tot}$ must be between 15 GeV and 70 GeV\n",
    " - Reconstructed transverse total momentum $p_{\\text{tot},T}$ must be between 40 and 250 MeV\n",
    " - Missing mass squared (electron hypothesis) $m_\\text{miss}^2(e) < 0.01~\\text{GeV}^2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24afc8b7-e820-485d-8ac3-648233678855",
   "metadata": {},
   "source": [
    "## Criterions categories\n",
    "From the selections above we can roughly create some \"categories\" of criterion:\n",
    " - Trigger\n",
    " - Basic objects requirements\n",
    " - Object selection conditions\n",
    "     - Timing\n",
    "     - Track quality\n",
    "     - Charge\n",
    "     - Momentum\n",
    "     - Vertex\n",
    "     - Geometric acceptance\n",
    " - Additional requirements\n",
    " - Pile-up veto\n",
    " - Reconstructed quantities\n",
    "\n",
    "This can be useful to roughly compare the selections and identify differences. The table below summarises the selection according to these categories\n",
    "\n",
    "| Type | Sample | Trigger | Basic requirements | O. timing | O. quality | O. charge | O. momentum | O. vertex | O. Geo. Acc. | Add. Req. | Veto | Reco quantities |\n",
    "| --:    | --:    | :--:    | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n",
    "| 3-track | K3pi   | Control | One 3-track vertex | $|t_\\text{vtx} - t_\\text{trigger}| < 6~\\text{ns}$ | $d_{i,j} > 10~\\text{cm}$ | +1 | N/A | $104~m < Z_\\text{vtx} < 180~m$ & $\\chi^2<25$ | NewCHOD & 4 Straw | N/A | N/A | $|p_\\text{tot} - 75~\\text{GeV}|<3~\\text{GeV}$ & $490~\\text{MeV} < m_{3\\pi} < 497~\\text{MeV}$ |\n",
    "| 1-track | Kmu2   | Control | < 10 tracks | $|t_\\text{track} - t_\\text{trigger}| < 10~\\text{ns}$ | $\\chi^2 < 20$ & No other vertex | +1 | $5~\\text{GeV} < p < 70~\\text{GeV}$ | $120~m < Z_\\text{vtx} < 180~m$ & $\\text{CDA} < 40~mm$ | MUV3 & 4 Straw | Signal in 4 Straw and (CHOD or NewCHOD) & KTAG in-time & MUV3 in-time signal & $E/p < 0.2$ | LAV, IRC, SAC | $m_\\text{miss}^2(\\mu) < 0.02~\\text{GeV}^2$ |\n",
    "| 1-track & 2-clusters | K2pi   | Control | 1 track & 3 clusters | $|t_\\text{track}-t_{\\pi_0}| < 2~\\text{ns}$; Cluster: $|t_\\text{cluster} - t_\\text{trigger}|<20~\\text{ns}$ & $\\Delta t_{i,j}<5~\\text{ns}$ | Track: Not fake & $\\Delta p_\\text{raw,fit}<20~\\text{GeV}$; Cluster: $\\Delta d_\\text{dead-cell} > 20~\\text{mm}$ & Electromagnetic & Not associated to track & Isolated & $\\Delta d_{i,j} > 200~\\text{mm}$ | Track: +1 | Track: $5~\\text{GeV} < p < 70~\\text{GeV}$; Cluster: $E>1~\\text{GeV}$ & $2~\\text{GeV} < E_1 + E_2 < 75~\\text{GeV}$ | $105~m < Z_\\text{vtx} < 180~m$ & $\\text{CDA} < 30~mm$ | NewCHOD & 4 Straw | Track: no MUV3 &  $E/p<0.9$ | LAV, IRC, SAC | $460~\\text{MeV} < m_K < 520~\\text{MeV}$ & $m_\\text{miss}^2(\\pi) < 0.015~\\text{GeV}^2$ |\n",
    "| 1-track & 2-clusters | Kmu3 | Control | N/A | $|t_\\text{cluster} - t_\\text{track}| < 6~\\text{ns}$ | Track: $\\chi^2 < 20$ & MUV3 association; Cluster: $\\Delta d_\\text{cls,track} > 150~\\text{mm}$ | Track: +1 | Track: $5~\\text{GeV} < p < 50~\\text{GeV}$; Cluster: $E>2~\\text{GeV}$ | Track: $110~m < Z_\\text{ch. vtx} < 180~m$ & $\\text{CDA} < 25$; Cluster: $|Z_\\text{ch. vtx} - Z_\\text{neut. vtx}|<10~m$ | Track: NewCHOD & 4 Straw & LKr & MUV3 | No track within 10 ns | LAV, IRC, SAC | $15~\\text{GeV} < p_\\text{tot} < 70~\\text{GeV}$ & $40~\\text{MeV} < p_{\\text{tot},T} < 250~\\text{MeV}$ & $m_\\text{miss}^2(\\mu) < 0.01~\\text{GeV}^2$ |\n",
    "| 1-track & 2-clusters | Ke3 | Control | N/A | $|t_\\text{cluster} - t_\\text{track}| < 6~\\text{ns}$ | Track: $\\chi^2 < 20$ & No MUV3 association; Cluster: $\\Delta d_\\text{cls,track} > 150~\\text{mm}$ | Track: +1 | Track: $5~\\text{GeV} < p < 50~\\text{GeV}$; Cluster: $E>2~\\text{GeV}$ | Track: $110~m < Z_\\text{ch. vtx} < 180~m$ & $\\text{CDA} < 25$; Cluster: $|Z_\\text{ch. vtx} - Z_\\text{neut. vtx}|<10~m$ | Track: NewCHOD & 4 Straw & LKr & MUV3 | No track within 10 ns | LAV, IRC, SAC | $15~\\text{GeV} < p_\\text{tot} < 70~\\text{GeV}$ & $40~\\text{MeV} < p_{\\text{tot},T} < 250~\\text{MeV}$ & $m_\\text{miss}^2(e) < 0.01~\\text{GeV}^2$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f5f502-d990-4549-88b7-7024541bc75c",
   "metadata": {},
   "source": [
    "Even though all the selections seem to contain a similar set of conditions, the cut values are sometimes differing slightly. As we will eventually compare events type selected with different selections, it is best to harmonize the cut values. This will avoid introducing systematic uncertainties due to different behaviour and different agreement between MC and data in different ranges of the variables on which we introduce conditions.\n",
    "\n",
    "The conditions where we can see obvious differences that can be harmonized are in the following list. When common criterion are selected, we have to use the most constraining values as we cannot \"undo\" what was already applied before presenting these data.\n",
    " - $Z \\text{vtx}$: common range is between 120 m and 180 m\n",
    " - LKr distance between track and cluster: common limit of at least 150 mm\n",
    " - LKr distance between clusters: common limit of at least 200 mm\n",
    " - Cluster Energy: common limit of at least 2 GeV\n",
    " - Vertex CDA: common limit of less than 25 mm\n",
    " - Distance between charged and neutral vertices: common limit of less than 10 m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c27608-6aa2-4a29-b9db-fe09b7127e68",
   "metadata": {},
   "source": [
    "## Technical aside \n",
    "### Functional cuts\n",
    "As you have seen, we are working in this project with pandas Dataframes. Among other things the dataframe allow us to easily implement cuts, which you have experienced already in the previous notebooks. The way we have done this so far is an operation done in multiple steps (example selecting a track momentum range):\n",
    " - Identify all events satisfying the lower range: `low_cond = df[\"track1_momentum_mag\"]>15000`\n",
    " - Identify all events satisfying the upper range: `upper_cond = df[\"track1_momentum_mag\"]<40000`\n",
    " - Combine both criterion: `cond = low_cond & upper_cond`\n",
    " - Select events satisfying both conditions: `df.loc[cond]`\n",
    "\n",
    "The first three steps actually creates a boolean array with one entry for each event: True or False. Only the last step actually filter out the events we are not interested in. We often shortened this in a single line:\n",
    "```python\n",
    "df.loc[(df[\"track1_momentum_mag\"]>15000) & (df[\"track1_momentum_mag\"]<40000)]\n",
    "df.loc[(df[\"track2_momentum_mag\"]>15000) & (df[\"track2_momentum_mag\"]<40000)]\n",
    "df.loc[(df[\"track3_momentum_mag\"]>15000) & (df[\"track3_momentum_mag\"]<40000)]\n",
    "```\n",
    "\n",
    "However we find that these lines are a bit verbose with a lot of redundancy: `df` is present three times, `track1_momentum_mag` twice. It also lacks flexibility: if we want to apply this condition on all three tracks, we have to write the line three times, with two small changes. Also if we want to try a different value of pmin and pmax, we may have to change this in many places, risking to forget changing it in one place. This could be improved by writing functions and loops, but the code would loose readability. When you read a selection code, you do not care how you applied a momentum cut to three tracks, only that you did and which cut values were used.\n",
    "\n",
    "Instead we can take advantage of one functionality provided by pandas: the argument of the `.loc()` function can be a `Callable` (i.e. a function or a lambda) taking a single argument (the dataframe) and returning a boolean array used for the filtering. We can therefore use it like this:\n",
    "```python\n",
    "def t1_momentum_cut_15_40(df):\n",
    "    return (df[\"track1_momentum_mag\"]>15000) & (df[\"track1_momentum_mag\"]<40000)\n",
    "def t2_momentum_cut_15_40(df):\n",
    "    return (df[\"track2_momentum_mag\"]>15000) & (df[\"track2_momentum_mag\"]<40000)\n",
    "def t3_momentum_cut_15_40(df):\n",
    "    return (df[\"track3_momentum_mag\"]>15000) & (df[\"track3_momentum_mag\"]<40000)\n",
    "\n",
    "df.loc[t1_momentum_cut_15_40]\n",
    "df.loc[t2_momentum_cut_15_40]\n",
    "df.loc[t3_momentum_cut_15_40]\n",
    "```\n",
    "\n",
    "This is so far not much of an improvement, except for the case where we use these conditions in multiple places. In that case we have to change only one function per track. We can another improvement by using another python propertie: we can write function that create functions!\n",
    "```python\n",
    "def make_momentum_cut(min_p, max_p, which_object = None):\n",
    "    # We can do some pre-processing\n",
    "    if which_object is None:\n",
    "        which_object = \"\"\n",
    "    else:\n",
    "        which_object = f\"{which_object}_\"\n",
    "\n",
    "    # We define the function from the example above\n",
    "    def cut(df):\n",
    "        # This function also works if we provide directly the series!\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            serie_cut = df[f\"{which_object}momentum_mag\"]\n",
    "        else:\n",
    "            serie_cut = df\n",
    "        # We implement the case where we don't have a lower or an upper limit\n",
    "        min_momentum_range = serie_cut > min_p if min_p else True\n",
    "        max_momentum_range = serie_cut < max_p if max_p else True\n",
    "        return min_momentum_range & max_momentum_range\n",
    "    # Return the function we created\n",
    "    return cut\n",
    "\n",
    "from functools import partial\n",
    "# Create a new function that binds the min/max parameters, but not which_object\n",
    "momentum_cut_15_40 = partial(make_momentum_cut, 15000, 40000)\n",
    "\n",
    "df.loc[momentum_cut_15_40(\"track1\")]\n",
    "df.loc[momentum_cut_15_40(\"track2\")]\n",
    "df.loc[momentum_cut_15_40(\"track3\")]\n",
    "```\n",
    "\n",
    "True, the overal code is longer, but the on the other hand the interesting part (that last 4 lines) are now much easier to read (reading the line we understand immediately what condition we apply), less error prone (we have a single place to change if we want to modify the cut everywhere), and more flexible (with the same function we can apply to any track without redefinition, and we can easily define another cut in a single line). The `make_momentum_cut` function is implemented in na62.hlf taking even further advantage of those principles by leveraging a generic *min_max* cut function. It is now very short:\n",
    "```python\n",
    "def make_momentum_cut(min_p, max_p, which_object = None) -> Callable:\n",
    "    return make_min_max_cut(min_p, max_p, which_value=\"momentum_mag\", which_object=which_object)\n",
    "```\n",
    "\n",
    "Many \"make_*_cut\" condition functions have been defined in such a way in the na62.hlf module. Have a look at the documentation to see which are available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b04ad9c-b7df-42c4-a8b0-2e1cd814171a",
   "metadata": {},
   "source": [
    "### Data/MC histograms\n",
    "Because we now have MC samples, what we will want to do is to compare our data with the simulation. This is done by plotting together the data and the MC on a single plot. There are however a few things to take into accout:\n",
    " - The data comprise the sum of all channels, therefore the MC contributions should be stacked (plotted on **top** of each other and not besides each other).\n",
    " - The MC events available for each channel do not represent the same number of kaons. Three parameters have to be taken into account to be able to normalize them relative to each other:\n",
    "   - The \"norm\" parameters retrieved in the file. It is the total number of events that were generated (how many kaon decays). Using these we can take into account their relative size (in terms of generation).\n",
    "   - The branching fraction of each sample.\n",
    "   - The selection acceptance/efficiency of each sample. The selection cuts do not have the same effect on each sample (because their kinematic distributions are different). \n",
    " - Once the MC samples are correctly normalized relative to each other, the total integrated MC sample must also be normalized to correspond to the number of data events that we have collected.\n",
    "\n",
    "**Exercise**: Imagine that you simulate 1000 kmu2 events out of which 500 pass your selection, and 2000 K3pi events out of which 100 pass your selection. How many kaons does that corresponds too for each sample? What should be their relative weights? If we select a data sample corresponding to 10000 events (consisting only of kmu2 and k3pi - your selection is very pure), how many kmu2 and k3pi does that correspond to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616dd82e-b629-4fbb-8ea9-85ea1355c5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_kmu2 = 1000\n",
    "n_k3pi = 2000\n",
    "n_kmu2_sel = 500\n",
    "n_k3pi_sel = 100\n",
    "ndata_sel = 10000\n",
    "\n",
    "# First compute to number of kaons that correspond to those simulated events\n",
    "n_kmu2_kaons = n_kmu2 / constants.kaon_br_map[\"kmu2\"]\n",
    "n_k3pi_kaons = n_k3pi / constants.kaon_br_map[\"kmu2\"]\n",
    "\n",
    "print(f\"Number of Kaons for kmu2: {n_kmu2_kaons:.2f}\")\n",
    "print(f\"Number of Kaons for kmu2: {n_k3pi_kaons:.2f}\")\n",
    "print(f\"Relative weight: {n_k3pi_kaons/n_kmu2_kaons:.2f} (1 k3pi kaon is worth {n_k3pi_kaons/n_kmu2_kaons:.0f} kmu2 kaons)\")\n",
    "\n",
    "# Now compute the weights for each selected event\n",
    "w_k3pi = (n_k3pi_sel/n_k3pi_kaons)\n",
    "w_kmu2 = (n_kmu2_sel/n_kmu2_kaons)\n",
    "print(f\"Relative selected weight: {w_k3pi/w_kmu2:.2f} (1 selected k3pi kaon is worth {w_k3pi/w_kmu2:.0f} selected kmu2 kaons)\")\n",
    "\n",
    "mc_normalization = w_k3pi + w_kmu2\n",
    "\n",
    "print(f\"Number of Kmu2 in the data: {ndata_sel*w_kmu2/mc_normalization:.0f}\")\n",
    "print(f\"Number of K3pi in the data: {ndata_sel*w_k3pi/mc_normalization:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa94f36e-cdb8-4c36-a7dc-b4856557eed7",
   "metadata": {},
   "source": [
    "There are two ways to normalize your MC histograms to the data.\n",
    " 1. Make sure that your MC histograms are correctly normalized relative to each other, then scale them all so that their integral is the same as the integral of the data. This method is simpler to apply early as it only requires a data histogram and MC histograms without additional ingredients. However this assumes that the events in all the histograms have been rigorously selected in the exact same way. Else the integral will be identical but some features may look very different.\n",
    " 2. Implement a normalization selection (see later in the notebook) to determine a kaon flux (how many kaons entered the experiment) and scale everything relative to that number. This has the advantage that this number is valid whatever selection criterion are applied. This will allow us to better compare data and MC and clearly see features that are not conssistent between the two, without having it diluted through the whole histogram.\n",
    "    \n",
    "**Example**: To illustrate the two methods above, we are going to generate a dataset according to a complex distribution (representing data), and another dataset with a slightly different distribution (representing MC). We are then going to produce histograms according to both methods discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1094c4ee-a370-4e49-ae4e-df974d211605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define our test distributions: \n",
    "#  1. two gaussian with completely different parameters on top of a uniform distribution. The amplitude of the second gaussian can be modified from parameter.\n",
    "#  2. One simple gaussian\n",
    "# These distribution will correspond to two \"channel\" featuring different behaviour.\n",
    "from lmfit.models import GaussianModel, LinearModel\n",
    "def test_distribution1(x, amp2=100):\n",
    "    # Create the model\n",
    "    distrib_model = GaussianModel(prefix=\"g1_\") + GaussianModel(prefix=\"g2_\") + LinearModel()\n",
    "\n",
    "    # Set the parameters\n",
    "    pars = distrib_model.make_params()\n",
    "    pars[\"g1_center\"].value = 2\n",
    "    pars[\"g1_sigma\"].value = 0.5\n",
    "    pars[\"g1_amplitude\"].value = 60\n",
    "    pars[\"g2_center\"].value = 5\n",
    "    pars[\"g2_amplitude\"].value = amp2\n",
    "    pars[\"intercept\"].value = 30\n",
    "    pars[\"slope\"].value = -3\n",
    "\n",
    "    # Return the model evaluated on the provided x values\n",
    "    return distrib_model.eval(x=x, params=pars)\n",
    "\n",
    "def test_distribution2(x):\n",
    "    # Create the model\n",
    "    distrib_model = GaussianModel(prefix=\"g1_\")\n",
    "\n",
    "    # Set the parameters\n",
    "    pars = distrib_model.make_params()\n",
    "    pars[\"g1_center\"].value = 2\n",
    "    pars[\"g1_sigma\"].value = 0.5\n",
    "    pars[\"g1_amplitude\"].value = 20\n",
    "\n",
    "    # Return the model evaluated on the provided x values\n",
    "    return distrib_model.eval(x=x, params=pars)\n",
    "    \n",
    "# We will then randomly pick a dataset corresponding to our distribution. We can do this \n",
    "# using the accept-reject method\n",
    "def accept_reject(distrib, range, max, n_samples, plot_it=False):\n",
    "    # Generate x and y uniformly distributed in a 2D box\n",
    "    x = np.random.uniform(range[0], range[1], n_samples)\n",
    "    y = np.random.uniform(0, max, n_samples)\n",
    "    # Compute for each x the expected y value according to the input distribution\n",
    "    y_test = distrib(x)\n",
    "\n",
    "    # Accept only the values which are below the curve\n",
    "    accept = np.where(y<=y_test)\n",
    "    reject = np.where(y>y_test)\n",
    "\n",
    "    # We can plot to illustrate\n",
    "    if plot_it:\n",
    "        plt.scatter(x[reject], y[reject], marker=\".\", label=\"Rejected\")\n",
    "        plt.scatter(x[accept], y[accept], marker=\".\", label=\"Accepted\")\n",
    "        plt.scatter(x, y_test, marker=\"x\", label=\"Exact distribution\")\n",
    "        plt.legend(loc=\"lower left\")\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"y\")\n",
    "    return x[accept]\n",
    "\n",
    "distrib_max = 100 # For accept-reject method\n",
    "# We are going to generate data corresponding to a certain flux, and a number of MC that we fix (to have something equivalent to our physics problem with data and MC).\n",
    "# We are also going to simulate the fact that each MC sample has a different branching ratio (0.75 and 0.25)\n",
    "flux = 100000\n",
    "n_simu = 200000\n",
    "n_simu2 = 150000\n",
    "br1 = 0.75\n",
    "br2 = 0.25\n",
    "\n",
    "x = np.arange(0,10,0.1)\n",
    "ratio = 0.7528 # Don't worry about this number. It's a trick to keep the BR of the distribution1 the same after changing the amplitude of the second gaussian\n",
    "\n",
    "# Generate the data\n",
    "data = accept_reject(lambda x: test_distribution1(x) + test_distribution2(x), (0,10), distrib_max, flux, plot_it=True)\n",
    "# Generate the MC twice using the first distribution: once with the same distribution as the data, once changing the amplitude of the second Gaussian \n",
    "mc_same = accept_reject(test_distribution1, (0,10), distrib_max*br1, n_simu)\n",
    "plt.figure()\n",
    "mc = accept_reject(partial(test_distribution1, amp2=50), (0,10), distrib_max*ratio, n_simu)\n",
    "# And once for the second distribution\n",
    "mc2 = accept_reject(test_distribution2, (0,10), distrib_max*br2, n_simu2)\n",
    "# Then we do a third sample from the one with the same distribution as the data, but we remove all the events \n",
    "# with x<1 (this would correspond to an additional cut in a data selection)\n",
    "mc_same_with_cut = mc_same[mc_same>1]\n",
    "mc2_with_cut = mc2[mc2>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00408af9-e55b-4448-94e8-ffd9b4299e96",
   "metadata": {},
   "source": [
    "First please notice the nice illustration of the \"Accept-Reject\" sampling method, where we randomly generate points in a 2D space and reject all those that are above the theoretical curve that we defined.\n",
    "\n",
    "Now below let's histogram our data. Three histograms according to the first histograming method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b424fe9c-f7b4-452d-94f2-5a1855b36e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(21,5))\n",
    "weights = [0.75/n_simu, 0.25/n_simu2]\n",
    "histo.hist_data(data, bins=100, range=(0,10), ax=ax[0])\n",
    "histo.stack_mc_scale([mc_same, mc2], bins=100, range=(0,10), ndata=len(data), weights=weights, ax=ax[0])\n",
    "ax[0].set_title(\"MC generated according to same distribution as data\")\n",
    "ax[0].set_xlabel(\"x\")\n",
    "\n",
    "histo.hist_data(data, bins=100, range=(0,10), ax=ax[1])\n",
    "histo.stack_mc_scale([mc_same_with_cut, mc2_with_cut], bins=100, range=(0,10), ndata=len(data), weights=weights, ax=ax[1])\n",
    "ax[1].set_title(\"MC generated according to same distribution as data\\n, wihth cut x>1\")\n",
    "ax[1].set_xlabel(\"x\")\n",
    "\n",
    "histo.hist_data(data, bins=100, range=(0,10), ax=ax[2])\n",
    "histo.stack_mc_scale([mc, mc2], bins=100, range=(0,10), ndata=len(data), weights=weights, ax=ax[2])\n",
    "ax[2].set_title(\"MC generated according to distribution where\\n the amplitude of the second sigma is different\")\n",
    "ax[2].set_xlabel(\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0398903-511e-4ea0-b26b-bf723e98e36a",
   "metadata": {},
   "source": [
    "On these plots, we can clearly see the disadvantave of this normalization technique:\n",
    " - Everything looks fine on the first plot, which has been carefully engineered to have MC samples corresponding to the data\n",
    " - On the second plot, we added an additional cut to the MC and as a result, the entirety of the MC histograms are shifted upwards.\n",
    " - The the last plot, the largest MC contribution is actually slightly different from the data (the second gaussian is a bit smaller) and as a result the whole plot looks bad, including the first Gaussian which is actually correct in MC.\n",
    "\n",
    "On the other hand these plots have been normalized easily, with the only required input being the BR and the number of generated MC samples. Those values are generally easy to use as they are provided externally (you know the BR of your kaon decay channels, and you know how many events you simulated).\n",
    "\n",
    "Now for the second normalization technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b189c03b-412d-41ab-83f9-b3f3871d6041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `stack_mc_flux` function uses internally the `constants.kaon_br_map` for the BR of each sample. So let's introduce fake channels with the BR that we used\n",
    "# for the test distributions\n",
    "constants.kaon_br_map[\"test1\"] = 0.75\n",
    "constants.kaon_br_map[\"test2\"] = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(21,5))\n",
    "histo.hist_data(data, bins=100, range=(0,10), ax=ax[0])\n",
    "histo.stack_mc_flux({\"test1\": mc_same, \"test2\": mc2}, {\"test1\": n_simu, \"test2\": n_simu2}, bins=100, range=(0,10), kaon_flux=flux, ax=ax[0])\n",
    "ax[0].set_title(\"MC generated according to same distribution as data\")\n",
    "ax[0].set_xlabel(\"x\")\n",
    "\n",
    "histo.hist_data(data, bins=100, range=(0,10), ax=ax[1])\n",
    "histo.stack_mc_flux({\"test1\": mc_same_with_cut, \"test2\": mc2_with_cut}, {\"test1\": n_simu, \"test2\": n_simu2}, bins=100, range=(0,10), kaon_flux=flux, ax=ax[1])\n",
    "ax[1].set_title(\"MC generated according to same distribution as data\\n, wihth cut x>1\")\n",
    "ax[1].set_xlabel(\"x\")\n",
    "\n",
    "histo.hist_data(data, bins=100, range=(0,10), ax=ax[2])\n",
    "histo.stack_mc_flux({\"test1\": mc, \"test2\": mc2}, {\"test1\": n_simu, \"test2\": n_simu2}, bins=100, range=(0,10), kaon_flux=flux, ax=ax[2])\n",
    "ax[2].set_title(\"MC generated according to distribution where\\n the amplitude of the second sigma is different\")\n",
    "ax[2].set_xlabel(\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472982a4-652b-45af-898c-93b719d2aaa3",
   "metadata": {},
   "source": [
    "All the plots produced using this second method look better:\n",
    " - The first plot is obviously OK as the MC model correspond to the data model\n",
    " - The second plot also look good. The results produced by this normalization method does not change if the cuts applied to data and MC are not exaclty the same. The parts where they are will look good (x>1)\n",
    " - Aside from the second Gaussian, which is different by design, the rest of the distribution is correctly normalized. In particular the first gaussian still looks OK, as it should because its parameters are the same as in data.\n",
    "\n",
    "On the other hand we needed a bit more information to produce these plots: the number of simulated MC samples and the BR (as for the previous method), but also the kaon flux. For this example, this number was just given as input (because this is a made up example), but generally this number is unknown and depends on the data sample. A complex data seletcion must be performed to estimate it, including the estimation of the uncertainty on this number. \n",
    "\n",
    "Any serious data analysis will however have to ultimately use this second technique, while the first one can be useful to produce some quick, preliminary plots. The obtention of the kaon flux number will be the topic of one of the next section of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53020025-99a9-4cbc-a4c9-efe0d050d7e2",
   "metadata": {},
   "source": [
    "## Selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03da3a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data, but also the MC simulations we have for each channel\n",
    "# N.B. Contrary to what we were doing in previous notebooks, we are now\n",
    "#      retrieving the second output value from import_root_files.\n",
    "data, data_norm = prepare.import_root_files([\"data/run12450.root\"])\n",
    "k2pi, k2pi_norm = prepare.import_root_files([\"data/k2pi.root\"])\n",
    "k3pi, k3pi_norm = prepare.import_root_files([\"data/k3pi.root\"])\n",
    "kmu2, kmu2_norm = prepare.import_root_files([\"data/kmu2.root\"])\n",
    "kmu3, kmu3_norm = prepare.import_root_files([\"data/kmu3.root\"])\n",
    "ke3, ke3_norm = prepare.import_root_files([\"data/ke3.root\"])\n",
    "\n",
    "# And we create a dictionary with the normalization parameters that we retrieved\n",
    "normalization_dict = {\"k2pi\": k2pi_norm, \"k3pi\": k3pi_norm, \"kmu2\": kmu2_norm, \"kmu3\": kmu3_norm, \"ke3\": ke3_norm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc2e6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the weights that we will need for plotting\n",
    "total_data = len(data)\n",
    "weights = histo.compute_samples_weights(normalization_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f1e9ea-9711-4669-96ed-6e0ef6ff1199",
   "metadata": {},
   "source": [
    "### One track selection\n",
    "As you can see, we have several channels (all except K3pi) which feature only one single track in the final state. It therefore makes sense to try to have a common selection allowing us to select those channels in a similar way. Later on this has the advantage that the comparison between samples from each channel will have similar systematic errors, because the selection criterion are as similar as possible.\n",
    "\n",
    "*Note*: All examples in this section will use the first method of normalization (scaling to data) because we do not have a normalization selection yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a544ba29-dd68-45de-9982-296b5e1f5dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first create a function that will plot the invariant mass for the data and MC passed, this will save us a lot of code later\n",
    "from typing import Dict, Tuple, List, Callable\n",
    "def plot_invariant_mass(data: pd.DataFrame, mc_dict: Dict[str, pd.DataFrame], weights_dict: Dict[str, float], \n",
    "                        mass_assignment: Dict) -> Tuple[int, int]:\n",
    "    # Plot the histogram for data\n",
    "    ndata = histo.hist_data(hlf.invariant_mass(data, mass_assignment), bins=400, range=(200,600))\n",
    "\n",
    "    # Plot the histogram stack for the MC samples\n",
    "    nmc = histo.stack_mc_scale([hlf.invariant_mass(mc_dict[mc_name], mass_assignment) for mc_name in mc_dict.keys()], \n",
    "        bins=400, range=(200, 600), weights=weights_dict, labels=mc_dict.keys(), ndata=ndata)\n",
    "\n",
    "    # Some default display parameters\n",
    "    plt.legend()\n",
    "    plt.yscale(\"log\")\n",
    "    plt.ylim(bottom=0.8)\n",
    "\n",
    "    # Return the number of data events plotted and a dictionary of number of MC events plotted by sample\n",
    "    return ndata, nmc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fbec36-c067-412a-87e3-b0016591ec53",
   "metadata": {},
   "source": [
    "Let's start first by selecting all single-track with two-cluster events. From this we can plot the invariant mass. In the following we will work in the pion hypothesis (considering k2pi as our test case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f8b3de-0d91-4074-a486-b1a80dd63481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the k2pi assumption\n",
    "k2pi_mass_assignment = {\"track1\": constants.pion_charged_mass, \"cluster1\": constants.photon_mass, \"cluster2\": constants.photon_mass}\n",
    "plot_invariant_mass_k2pi = partial(plot_invariant_mass, mass_assignment=k2pi_mass_assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0668d39-9c33-471b-86c1-2de5303ce4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndata, nmc = plot_invariant_mass_k2pi(data, {\"k3pi\": k3pi, \"kmu2\": kmu2, \"k2pi\": k2pi, \"kmu3\": kmu3, \"ke3\": ke3}, weights)\n",
    "plt.title(\"Reconstructed $\\pi^+\\gamma\\gamma$ invariant mass\")\n",
    "plt.xlabel(\"$m_{\\pi^+\\gamma\\gamma}$ [MeV/$c^2$]\")\n",
    "print(f\"K2pi selection purity: {nmc['k2pi']/sum(nmc.values()):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95155fb8-c6ed-46c2-8bbc-a901b0a1c493",
   "metadata": {},
   "source": [
    "The Data/MC agreement does not look good. The reason is that we use an inconsistent mix of various pre-selections with various efficiencies depending on the sample. We did not even select events with 1 Track and 2 clusters, which means we have really included in the plot random events.\n",
    "\n",
    "Furthermore as already mentioned, the pre-selections have a set of similar cuts, but not necessarily at the same values. We will therefore reconcile these cuts to a common value as described earlier. We define hereafter the list of cuts that can be applied to all (1-track & 2 cluster) selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc0f073-386c-4002-818e-7b29528de1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the correct topology\n",
    "cond_1T2C = hlf.make_exists_cut([\"track1\", \"cluster1\", \"cluster2\"], [])\n",
    "\n",
    "# Distance/position cuts\n",
    "lkr_dtrack_cluster_cond = partial(hlf.make_lkr_distance_cut, 150, None)\n",
    "lkr_dclusters_cond = hlf.make_lkr_distance_cut(200, None, \"cluster1\", \"cluster2\")\n",
    "z_vertex_cond = hlf.make_z_vertex_cut(120000, 180000)\n",
    "cda_cond = hlf.make_cda_cut(None, 25)\n",
    "neutral_vtx_cond = hlf.make_charged_neutral_vertex_cut(None, 10000, \"cluster1\", \"cluster2\", constants.pion_neutral_mass)\n",
    "\n",
    "# Energy/momentum cuts\n",
    "cluster_energy_cond = partial(hlf.make_energy_cut, 2000, None)\n",
    "\n",
    "# PID cuts\n",
    "rich_e_cond = partial(hlf.make_rich_cut, constants.rich_hypothesis_map[\"e\"])\n",
    "rich_pi_cond = partial(hlf.make_rich_cut, constants.rich_hypothesis_map[\"pi\"])\n",
    "rich_mu_cond = partial(hlf.make_rich_cut, constants.rich_hypothesis_map[\"mu\"])\n",
    "lkr_e_cond = partial(hlf.make_eop_cut, 0.95, 1.05)\n",
    "lkr_pi_cond = partial(hlf.make_eop_cut, None, 0.9)\n",
    "lkr_mu_cond = partial(hlf.make_eop_cut, None, 0.2)\n",
    "muv3_mu_cond = partial(hlf.make_muv3_cut, True, time_window=1.5)\n",
    "muv3_not_mu_cond = partial(hlf.make_muv3_cut, False, time_window=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dbcdd2-1594-47cd-89f1-6a6a30e067f9",
   "metadata": {},
   "source": [
    "Then we restart the plot selecting at least the correct topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29571c23-6fc6-4286-84c3-d97b10b4ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sel, mc_sel = hlf.select_all(data, {\"k3pi\": k3pi, \"kmu2\": kmu2, \"k2pi\": k2pi, \"kmu3\": kmu3, \"ke3\": ke3}, [cond_1T2C])\n",
    "ndata, nmc = plot_invariant_mass_k2pi(data_sel, mc_sel, weights)\n",
    "plt.title(\"Reconstructed $\\pi^+\\gamma\\gamma$ invariant mass\")\n",
    "plt.xlabel(\"$m_{\\pi^+\\gamma\\gamma}$ [MeV/$c^2$]\")\n",
    "print(f\"K2pi selection purity: {nmc['k2pi']/sum(nmc.values()):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4eab64-3b7c-4da7-a961-5b90439f107c",
   "metadata": {},
   "source": [
    "This is already better, but we still have issues. Let's apply the remaining cuts to align the selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad16f1bb-497c-4080-ae4e-94fdcb82f079",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sel, mc_sel = hlf.select_all(data_sel, mc_sel, \n",
    "                                  [z_vertex_cond, lkr_dtrack_cluster_cond(\"track1\", \"cluster1\"), lkr_dtrack_cluster_cond(\"track1\", \"cluster2\"),\n",
    "                                   lkr_dclusters_cond, cluster_energy_cond(\"cluster1\"), cluster_energy_cond(\"cluster2\"), \n",
    "                                   cda_cond, neutral_vtx_cond])\n",
    "ndata, nmc = plot_invariant_mass_k2pi(data_sel, mc_sel, weights)\n",
    "plt.title(\"Reconstructed $\\pi^+\\gamma\\gamma$ invariant mass\")\n",
    "plt.xlabel(\"$m_{\\pi^+\\gamma\\gamma}$ [MeV/$c^2$]\")\n",
    "print(f\"K2pi selection purity: {nmc['k2pi']/sum(nmc.values()):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d154856c-8d2d-49be-9976-179d4f0b5ac7",
   "metadata": {},
   "source": [
    "These cuts have not changed the picture much. We can confirm this by looking at the acceptance of each cut (i.e. which additional fraction of events the cut will let pass)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa54fef6-89e4-4f50-a30a-ee8fd77a7140",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_sel[\"k2pi\"].attrs[\"acceptances\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2293c9-170c-4440-8ca3-bc22b5f15a7f",
   "metadata": {},
   "source": [
    "We can see that indeed each cut has less than 1% effect. The reason however is clear: these cuts have already been applied in the pre-selection, we are just applying fine tuning here (like cutting at cda<25 instead of cda<30).\n",
    "\n",
    "There are still two major differences between the selections: the PID and the reconstructed kinematic cuts. At this point however we cannot remain generic and we have to actually select specific channels. We will embrace full on the k2pi selection and move to other selections later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c692a9-684c-464e-b7a6-6a0019513a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K2pi reconstructed kinematic cuts\n",
    "k2pi_mmiss2_cond = hlf.make_missing_mass_sqr_cut(min_mm2=None, max_mm2=0.015*1e6, mass_assignments=k2pi_mass_assignment) # In GeV\n",
    "k2pi_inv_mass_cond = hlf.make_invariant_mass_cut(min_mass=460, max_mass=520, mass_assignments=k2pi_mass_assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d6e967-fe0d-4309-a325-d044812c67e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply all PID techniques\n",
    "data_sel_pid, mc_sel_pid = hlf.select_all(data_sel, mc_sel, [lkr_pi_cond(which_track=\"track1\"), \n",
    "                                                             muv3_not_mu_cond(which_track=\"track1\"), \n",
    "                                                             rich_pi_cond(which_track=\"track1\")])\n",
    "ndata, nmc = plot_invariant_mass_k2pi(data_sel_pid, mc_sel_pid, weights)\n",
    "plt.ylim(bottom=1e-2)\n",
    "plt.title(\"Reconstructed $\\pi^+\\gamma\\gamma$ invariant mass\")\n",
    "plt.xlabel(\"$m_{\\pi^+\\gamma\\gamma}$ [MeV/$c^2$]\")\n",
    "print(f\"K2pi selection pollution (1-purity): {1-nmc['k2pi']/sum(nmc.values()):.2e}\")\n",
    "nmc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d8ac24-7a6c-47bc-8aaf-5d2e67d2ef0b",
   "metadata": {},
   "source": [
    "Much better, but we can still see a total of ~0.84 estimated background events mostly due to k3pi (for a pollution at the order of $4.5\\times10^{-5}$ which is already very good). We can further improve by adding the final kinematic cuts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afbc51c-b106-4340-b275-930f1c1be696",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sel_pid, mc_sel_pid = hlf.select_all(data_sel_pid, mc_sel_pid, [k2pi_mmiss2_cond, k2pi_inv_mass_cond])\n",
    "ndata, nmc = plot_invariant_mass_k2pi(data_sel_pid, mc_sel_pid, weights)\n",
    "plt.ylim(bottom=1e-2)\n",
    "plt.title(\"Reconstructed $\\pi^+\\gamma\\gamma$ invariant mass\")\n",
    "plt.xlabel(\"$m_{\\pi^+\\gamma\\gamma}$ [MeV/$c^2$]\")\n",
    "print(f\"K2pi selection pollution (1-purity): {1-nmc['k2pi']/sum(nmc.values()):.2e}\")\n",
    "nmc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab15d2e-475f-41a0-94e6-4d2654765ad0",
   "metadata": {},
   "source": [
    "This last operation gained us another order of magnitude in purity. But what about the acceptance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1f5d2e-3e6c-479e-97ad-f5831dd2703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mc_sel_pid[\"k2pi\"].attrs[\"acceptances\"])\n",
    "print(f\"K2pi selection acceptance: {len(mc_sel_pid['k2pi'])/normalization_dict['k2pi']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7914e0-aef6-46e9-8aee-8a62abc2a658",
   "metadata": {},
   "source": [
    "We see here that the total selection acceptance is 9.08%, which means that out of all K2pi events simulated, only 9.08% are making it to the end of the selection. This seems not a lot but that is in fact the typical perforformances of a selection in NA62 if we want a pure sample. The acceptance of each individual cut that we have applied is high, and as already mentioned the reason is that most of them have already been applied in the pre-selection and their full performance is actually included in the original 15.7% acceptance of the pre-selection. \n",
    "\n",
    "What stands out is that two cuts in particular have a large impact: \n",
    " - The cut on the z vertex. In fact moving from 105 m to 120 m reduces the allowed decay volume by 20%. This translates to a 15% loss in acceptance (because of the kinematics and geometrical acceptance, the vertex Z distribution is not completely uniform - see next plot). This is a large loss, but unfortuntely a necessary one if we want to keep this selection as close as possible to the other 1-track & 2 clusters selections.\n",
    " - The RICH cut on the PID induces an enormous acceptance loss of 31.5% (relative). Let's see if if it is really necessary or if E/P and MUV3 is enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8ed697-83b2-4289-a60d-23a204e7b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Z vertex distribution of k2pi to illustrate the point made in the previous paragraph.\n",
    "k2pi[\"vtx_z\"].hist(bins=80, range=(100000, 180000))\n",
    "plt.title(\"K2pi sample vertex Z distribution\")\n",
    "plt.xlabel(\"$Z_\\mathrm{vtx}$ [mm]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07470319-495e-462b-9ff6-5c30c7fc1d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PID techniques but not RICH, directly apply also the reconstructed kinematics cuts\n",
    "data_sel_pid, mc_sel_pid = hlf.select_all(data_sel, mc_sel, [lkr_pi_cond(which_track=\"track1\"), muv3_not_mu_cond(which_track=\"track1\"), \n",
    "                                                             k2pi_mmiss2_cond, k2pi_inv_mass_cond])\n",
    "ndata, nmc = plot_invariant_mass_k2pi(data_sel_pid, mc_sel_pid, weights)\n",
    "plt.ylim(bottom=1e-2)\n",
    "plt.title(\"Reconstructed $\\pi^+\\gamma\\gamma$ invariant mass\")\n",
    "plt.xlabel(\"$m_{\\pi^+\\gamma\\gamma}$ [MeV/$c^2$]\")\n",
    "print(f\"K2pi selection pollution (1-purity): {1-nmc['k2pi']/sum(nmc.values()):.2e}\")\n",
    "nmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49e16ee-cec0-4ffe-af4a-aaa4af058173",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mc_sel_pid[\"k2pi\"].attrs[\"acceptances\"])\n",
    "print(f\"K2pi selection acceptance: {len(mc_sel_pid['k2pi'])/normalization_dict['k2pi']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf5765c-bb11-4799-a3a7-0ede805aeb0f",
   "metadata": {},
   "source": [
    "The selection acceptance has increased by 4% absolute. But on the other hand the purity has decreased by two orders of magnitude! Depending on our objective (purity vs. acceptance) we can decide to use this extra PID condition or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e5df9c-6b3a-4acf-be52-bb7a344f8583",
   "metadata": {},
   "source": [
    "### Normalization selection\n",
    "\n",
    "Let's assume in the following that we are interested in a pure selection, because we do not actually want to measure K2pi but only use it as normalization. In NA62 this process is necessary as we do not have an absolute measurement of the kaon flux (i.e. how many kaons enter our experiment). Instead we must use a well known normalization channel (such as K2pi), measure a flux based on a normalization selection (a pure selection for K2pi) and measure our signal channel relative to the normalization channel. We can then move from a relative value to an absolute value by using the K2pi BR as an external input (with its own uncertainties, which is why we want to use a channel with existing precision measurements).\n",
    "\n",
    "As an example, the process for measuring the Ke3 branching ratio using K2pi as normalization is the following:\n",
    " - Perform the K2pi and Ke3 selections, estimate the K2pi selection acceptance $A_\\text{k2pi}$ (on K2pi events) and Ke3 selection acceptance $A_\\text{ke3}$ (on Ke3 events).\n",
    " - We know that in general the number of selected events is $N_\\text{sel} = N_K *\\text{BR} * A * \\varepsilon_\\text{trigg}$ where $N_K$ is the kaon flux, BR is the branching fraction of the channel, $A$ is the selection acceptance, and $\\varepsilon_\\text{trigg}$ is the trigger efficiency.\n",
    " - Applying this to the K2pi normalization we have: $$N_K = \\frac{N_\\text{k2pi,sel}}{A_\\text{k2pi}\\cdot\\text{BR(k2pi)}\\cdot\\varepsilon_\\text{trigg,k2pi}}$$\n",
    " - We have the equivalent equation for Ke3, into which we can substitute $N_K$ estimated from K2pi: $$N_\\text{ke3,sel} = N_K\\cdot A_\\text{ke3}\\cdot\\text{BR(ke3)}\\cdot\\varepsilon_{trigg,ke3}$$\n",
    "$$\\Rightarrow\\text{BR(ke3)} = \\frac{N_\\text{ke3,sel}}{N_K\\cdot A_\\text{ke3}\\cdot\\varepsilon_{trigg,ke3}}$$\n",
    "$$\\Rightarrow\\text{BR(ke3)} = \\frac{N_\\text{ke3,sel}}{N_\\text{k2pi,sel}}\\cdot\\frac{A_\\text{k2pi}}{A_\\text{ke3}}\\cdot\\frac{\\varepsilon_\\text{trigg,k2pi}}{\\varepsilon_{trigg,ke3}}\\cdot\\text{BR(k2pi)}$$\n",
    "This last expression gives us our final measurement as a function of the external parameter BR(k2pi). As you can see we also take ratios of equivalent quantities between Ke3 and K2pi. This means that in the assumption that this quantities are obtained and apply in a similar way for both channels (selections are closely related, trigger lines are the same and applying on similar conditions, data are acquired at the same time, ...) then they will have similar systematic uncertainties and they will cancel out in the ratio at first order.\n",
    "\n",
    "In the following we will also assume that the trigger efficiencies are 100% (real value is very high anyways, well above 90%). Please note also that the Kaon flux computed is an **effective** flux, not the absolute one. It depends on the trigger line, on the trigger downscaling, on some geometry parameters, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2a4f49-19b6-4dae-af74-5a5f41b9ffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define our complete K2pi selection\n",
    "common_1track_2cluster_selection = [cond_1T2C, z_vertex_cond, \n",
    "                                    lkr_dtrack_cluster_cond(\"track1\", \"cluster1\"), lkr_dtrack_cluster_cond(\"track1\", \"cluster2\"), lkr_dclusters_cond, \n",
    "                                    cluster_energy_cond(\"cluster1\"), cluster_energy_cond(\"cluster2\"), \n",
    "                                    cda_cond, neutral_vtx_cond]\n",
    "\n",
    "k2pi_selection = [lkr_pi_cond(which_track=\"track1\"), muv3_not_mu_cond(which_track=\"track1\"), rich_pi_cond(which_track=\"track1\"), \n",
    "                  k2pi_mmiss2_cond, k2pi_inv_mass_cond]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c62f34-3031-4ba4-8975-a34e148a989e",
   "metadata": {},
   "source": [
    "Having defined our K2pi selection for normalization, we can compute the flux $N_\\text{k2pi,sel}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462bdf99-26bc-459d-bbf2-8641b1f3e4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select first common 1-track & 2-clusters sample so we can use it multiple times without re-selecting it\n",
    "data_1T2C, mc_1T2C = hlf.select_all(data, {\"k3pi\": k3pi, \"kmu2\": kmu2, \"k2pi\": k2pi, \"kmu3\": kmu3, \"ke3\": ke3}, common_1track_2cluster_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24208e2e-b53a-4650-bfcf-270139288672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the k2pi selection\n",
    "data_k2pi, mc_k2pi = hlf.select_all(data_1T2C, mc_1T2C, k2pi_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d941ca7-2dea-4040-81ee-c7614f9c974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_k2pi_mc_sel = len(mc_k2pi[\"k2pi\"])\n",
    "n_k2pi_data_sel = len(data_k2pi)\n",
    "k2pi_acc = n_k2pi_mc_sel/normalization_dict[\"k2pi\"]\n",
    "k2pi_N_K = n_k2pi_data_sel/(k2pi_acc * constants.kaon_br_map[\"k2pi\"] * 1.0)\n",
    "\n",
    "print(f\"K2pi selection acceptance: {k2pi_acc:.2%}\")\n",
    "print(f\"Number of K2pi candidates in data: {len(data_k2pi)} candidates\")\n",
    "print(f\"Kaon flux: {k2pi_N_K:.2e} kaons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de8d07a-4edb-4c4e-b0e7-0980a4d95bde",
   "metadata": {},
   "source": [
    "### Signal selection\n",
    "We are now interested in selecting the signal. For this exercise we are going to use Ke3 as the signal. As mentionned earlier we want to have our signal selection as close as possible to our normalization selection to cancel out the systematics in the ratio. We are therefore going to start with the common 1-track & 2-clusters selection, and add the few PID and reconstructed kinematic cuts specific to Ke3.\n",
    "\n",
    "Ke3 is an opened-kinematics channel, so we cannot use the reconstructed invariant mass to constrain it. But the missing mass squared must peak at 0, so we can use that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52efc2ca-0c78-46ce-a962-adbeb17c674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the missing mass squared plotting function as we did for the invariant mass earlier\n",
    "def plot_missing_mass2(data: pd.DataFrame, mc_dict: Dict[str, pd.DataFrame], kaon_flux: float, normalization: Dict[str, float],\n",
    "                        mass_assignment: Dict) -> Tuple[int, int]:\n",
    "    # Plot the histogram for data\n",
    "    ndata = histo.hist_data(hlf.missing_mass_sqr(data, mass_assignment), bins=100, range=(-50000, 100000))\n",
    "\n",
    "    # Plot the histogram stack for the MC samples\n",
    "    nmc = histo.stack_mc_flux({mc_name: hlf.missing_mass_sqr(mc_dict[mc_name], mass_assignment) for mc_name in mc_dict.keys()}, normalization,\n",
    "        bins=100, range=(-50000, 100000), labels=mc_dict.keys(), kaon_flux=kaon_flux)\n",
    "\n",
    "    # Some default display parameters\n",
    "    plt.legend()\n",
    "    plt.yscale(\"log\")\n",
    "    plt.ylim(bottom=0.8)\n",
    "\n",
    "    # Return the number of data events plotted and a dictionary of number of MC events plotted by sample\n",
    "    return ndata, nmc\n",
    "\n",
    "# Make the Ke3 assumption\n",
    "ke3_mass_assignment = {\"track1\": constants.electron_mass, \"cluster1\": constants.photon_mass, \"cluster2\": constants.photon_mass}\n",
    "plot_missing_mass2_ke3 = partial(plot_missing_mass2, mass_assignment=ke3_mass_assignment, kaon_flux=k2pi_N_K, normalization=normalization_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c81b2d-b5ac-462b-a90a-8c844d2de242",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndata, nmc = plot_missing_mass2_ke3(data_1T2C, mc_1T2C)\n",
    "plt.ylim(bottom=1e-2)\n",
    "plt.title(\"Missing mass squared (electron hypthesis)\")\n",
    "plt.xlabel(\"$m_\\mathrm{miss}^2(e)$ [$\\mathrm{MeV}^{2}/c^{4}]$\")\n",
    "print(f\"Ke3 selection pollution (1-purity): {1-nmc['ke3']/sum(nmc.values()):.2e}\")\n",
    "nmc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05280bdb-db1a-40db-a4af-ff752892e17a",
   "metadata": {},
   "source": [
    "This was the result of the common 1-track & 2-clusters selection, without applying yet any Ke3 specific cuts. \n",
    "\n",
    "There are two reasons for applying a cut:\n",
    " - Removing a phase space region including more background than signal. This is mostly what we have been doing so far.\n",
    " - Removing a region of phase space which is not reproduced well in MC, thus removing possible sources of systematic uncertainties in your final result.\n",
    "\n",
    "While performing an analysis, you will want to plot as many observables as possible, always comparing data and MC. Certainly you will want to check all those on which you are applying a cut. You want to make sure that the region that you keep agrees well between data and MC. Of course your initial data are polluted with many channels and effects not reproduced in MC, so you do not want to do your Data/MC comparison on the \"raw\" data without any selection cut. Very often, what you want to do is to apply all or most of your cuts, except the one you are checking.\n",
    "\n",
    "Let's prepare some code to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc23249-40e8-442b-97f4-1ff42482b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List, Dict, Callable\n",
    "# Create a function that will apply all the cuts to the data and MC, except a cut we want to check,\n",
    "# then plot the relevant distribution\n",
    "def plot_before_cut(data: pd.DataFrame, mc_dict: Dict[str, pd.DataFrame], \n",
    "                    selection: List[Callable], cut: Union[Callable, str], \n",
    "                    transform: Union[Callable, str], \n",
    "                    bins: int = 100, range: Union[None, Tuple[int, int]] = None, ax: Union[None, plt.Axes] = None) -> None:\n",
    "    # Cut can be either the cut function we want to remove, or the name of the cut function\n",
    "    # Example: z_vertex_cond = make_z_vertex_cut(105000, 180000)\n",
    "    #   - Call with cut=z_vertex_cond\n",
    "    #   - Or call with cut='z_vertex_cut'\n",
    "    # Transform indicates which/how the plotted value should be extracted from the dataframe\n",
    "    \n",
    "    truncated_selection = hlf.remove_cut(selection, cut)\n",
    "\n",
    "    # Perform truncated selection on data and MC\n",
    "    d, m = hlf.select_all(data, mc_dict, truncated_selection)\n",
    "\n",
    "    # Two cases: transform is a string and should correspond to a column of the dataframe, or a callable to be applied to the dataframe\n",
    "    if isinstance(transform, str):\n",
    "        # Transform it into a callable so we do not have to distinguish later\n",
    "        col_name = transform\n",
    "        transform = lambda df: df[col_name]\n",
    "    # And plot\n",
    "    histo.hist_data(transform(d), bins=bins, range=range, ax=ax)\n",
    "    # N.B. We use here two variables external to the function. This will work only in the notebook after we ran the cells generating the\n",
    "    # variables 'normalization_dict' and 'k2pi_N_K'\n",
    "    histo.stack_mc_flux({_: transform(m[_]) for _ in m}, normalization_dict, bins=bins, range=range, kaon_flux=k2pi_N_K, ax=ax)\n",
    "\n",
    "# Just for convenience later so we have less code to type\n",
    "data_orig = data\n",
    "mc_dict_orig = {\"k3pi\": k3pi, \"kmu2\": kmu2, \"k2pi\": k2pi, \"kmu3\": kmu3, \"ke3\": ke3}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee453a55-ed9c-48eb-8aaf-c71ceffb3c6f",
   "metadata": {},
   "source": [
    "Equipped with this, let's check the Data/MC agreement for the distributions on which we perform cuts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5195c055-7f7d-4fce-babf-4f21950b8af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z vertex cut\n",
    "plot_before_cut(data_orig, mc_dict_orig, common_1track_2cluster_selection, z_vertex_cond, \"vtx_z\", range=(100000, 180000))\n",
    "plt.xlabel(\"$Z_\\mathrm{vtx}$ [mm]\")\n",
    "plt.legend()\n",
    "plt.title(\"Vertex Z distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cec7d84-2d93-41eb-bcff-f3c59bc67fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LKr distance cut (3 different cuts: track-cl1, track-cl2, cl1-cl2)\n",
    "fig, ax = plt.subplots(1, 3, figsize=(21, 5))\n",
    "plot_before_cut(data_orig, mc_dict_orig, common_1track_2cluster_selection, \"lkr_distance_cut\", \n",
    "                partial(hlf.lkr_distance, object_1=\"track1\", object_2=\"cluster1\"), range=(0, 3500), ax=ax[0])\n",
    "ax[0].legend()\n",
    "ax[0].set_title(\"LKr distance: track-cluster1\")\n",
    "ax[0].set_xlabel(\"$d_\\mathrm{track,cluster}$ [mm]\")\n",
    "\n",
    "plot_before_cut(data_orig, mc_dict_orig, common_1track_2cluster_selection, \"lkr_distance_cut\", \n",
    "                partial(hlf.lkr_distance, object_1=\"track1\", object_2=\"cluster2\"), range=(0, 3500), ax=ax[1])\n",
    "ax[1].legend()\n",
    "ax[1].set_title(\"LKr distance: track-cluster2\")\n",
    "ax[1].set_xlabel(\"$d_\\mathrm{track,cluster}$ [mm]\")\n",
    "\n",
    "plot_before_cut(data_orig, mc_dict_orig, common_1track_2cluster_selection, \"lkr_distance_cut\", \n",
    "                partial(hlf.lkr_distance, object_1=\"cluster1\", object_2=\"cluster2\"), range=(0, 2000), ax=ax[2])\n",
    "ax[2].legend()\n",
    "ax[2].set_title(\"LKr distance: cluster1-cluster2\")\n",
    "ax[2].set_xlabel(\"$d_\\mathrm{cluster,cluster}$ [mm]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6403f19f-a78f-46b8-9879-381213a741f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster energy cuts\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "plot_before_cut(data_orig, mc_dict_orig, common_1track_2cluster_selection, \"energy_cut\", \"cluster1_lkr_energy\", range=(0, 60000), ax=ax[0])\n",
    "ax[0].legend()\n",
    "ax[0].set_title(\"Cluster energy\")\n",
    "ax[0].set_xlabel(\"E [MeV]\")\n",
    "\n",
    "plot_before_cut(data_orig, mc_dict_orig, common_1track_2cluster_selection, \"energy_cut\", \"cluster2_lkr_energy\", range=(0, 60000), ax=ax[1])\n",
    "ax[1].legend()\n",
    "ax[1].set_title(\"Cluster energy\")\n",
    "ax[1].set_xlabel(\"E [MeV]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2120b2a2-1d30-4c40-9f2d-0f90b1ddeef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CDA cut\n",
    "plot_before_cut(data_orig, mc_dict_orig, common_1track_2cluster_selection, cda_cond, hlf.beam_vertex_cda, range=(0,20))\n",
    "plt.xlabel(\"CDA [mm]\")\n",
    "plt.legend()\n",
    "plt.title(\"CDA distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4f011b-d3a2-4e57-b8ea-8fea004e1862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CDA cut\n",
    "plot_before_cut(data_orig, mc_dict_orig, common_1track_2cluster_selection, neutral_vtx_cond, \n",
    "                partial(hlf.charged_neutral_distance, cluster_1=\"cluster1\", cluster_2=\"cluster2\", clusters_invariant_mass=constants.pion_neutral_mass), range=(0,250000))\n",
    "plt.xlabel(\"$d_\\mathrm{neutral,charged}$ [mm]\")\n",
    "plt.legend()\n",
    "plt.title(\"Distance between charged and neutral vertex\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6042fe05-1e0c-4c89-a4d3-8c2ecd1c7b4b",
   "metadata": {},
   "source": [
    "We have gone here through all the common 1-track & 2-clusters cuts. The agreement between Data/MC looks very reasonable in all of the variables, with the possible exception of the CDA. We will keep this in mind for later and look for possible systematic effects due to this cut."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a792ca-fb20-422c-9ffc-cad37108ca2c",
   "metadata": {},
   "source": [
    "Let's complete our selection with PID and reconstructed kinematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd30c8f6-614f-460d-824e-ac65889c9e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define our specifc Ke3 selection\n",
    "ke3_mmiss2_cond = hlf.make_missing_mass_sqr_cut(min_mm2=None, max_mm2=0.01*1e6, mass_assignments=ke3_mass_assignment) # In GeV\n",
    "ptot_neutrino_cond = hlf.make_total_momentum_cut(15000, 70000)\n",
    "pt_neutrino_cond = hlf.make_transverse_momentum_cut(40, 250)\n",
    "  \n",
    "ke3_selection = [lkr_e_cond(which_track=\"track1\"), # No other PID conditions necessary here, E/p is very good for electrons\n",
    "                 ptot_neutrino_cond, pt_neutrino_cond, ke3_mmiss2_cond]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d3818f-d2c4-43b6-a927-7dc5181dcf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ke3, mc_ke3 = hlf.select_all(data_1T2C, mc_1T2C, ke3_selection)\n",
    "ndata, nmc = plot_missing_mass2_ke3(data_ke3, mc_ke3)\n",
    "plt.ylim(bottom=1e-2)\n",
    "plt.title(\"Missing mass squared (electron hypthesis)\")\n",
    "plt.xlabel(\"$m_\\mathrm{miss}^2(e)$ [$\\mathrm{MeV}^{2}/c^{4}]$\")\n",
    "print(f\"Ke3 selection pollution (1-purity): {1-nmc['ke3']/sum(nmc.values()):.2e}\")\n",
    "nmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f27859-f627-49c0-99d0-5ec18b9545cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mc_ke3[\"ke3\"].attrs[\"acceptances\"])\n",
    "print(f\"Ke3 selection acceptance: {len(mc_ke3['ke3'])/normalization_dict['ke3']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acf9399-c2b0-4f2e-807e-020ade1f6377",
   "metadata": {},
   "source": [
    "This gives us an acceptance of 7.4%, not too bad, for an excellent purity at the level of $6\\times10^{-5}$.\n",
    "\n",
    "Before continuing, let's check also the Ke3 Data/MC agreement in the distributions of the cuts specific to Ke3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc4e57c-25ad-4488-95d9-0c871bb83bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E/P cut\n",
    "plot_before_cut(data_1T2C, mc_1T2C, ke3_selection, \"eop_cut\", \"track1_eop\", range=(0,1.5))\n",
    "plt.xlabel(\"$E/p$\")\n",
    "plt.legend()\n",
    "plt.title(\"Track E/p\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765a1f62-a8dc-4728-bcf9-42be6ce28f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total momentum cut\n",
    "plot_before_cut(data_1T2C, mc_1T2C, ke3_selection, ptot_neutrino_cond, hlf.total_momentum, range=(0,80000))\n",
    "plt.xlabel(\"$p_\\mathrm{tot}$ [MeV/c]\")\n",
    "plt.legend()\n",
    "plt.title(\"Total reconstructed momentum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619542f9-c0e3-4d9f-a0d1-890d1398a49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transverse momentum cut\n",
    "plot_before_cut(data_1T2C, mc_1T2C, ke3_selection, pt_neutrino_cond, hlf.transverse_momentum_beam, range=(0,250))\n",
    "plt.xlabel(\"$p_\\mathrm{t}$ [MeV/c]\")\n",
    "plt.legend()\n",
    "plt.title(\"Total transverse momentum with respect to Beam momentum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b54ac84-2f5b-408d-99b9-9e24f5b2a4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing mass squared cut\n",
    "plot_before_cut(data_1T2C, mc_1T2C, ke3_selection, ke3_mmiss2_cond, partial(hlf.missing_mass_sqr, momenta_or_masses=ke3_mass_assignment), range=(-20000,20000))\n",
    "plt.xlabel(\"$m_\\mathrm{miss}^2(e)$ [$\\mathrm{MeV}^2/c^{4}$]\")\n",
    "plt.legend()\n",
    "plt.title(\"Missing mass squared (electron hypothesis)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9989af-001b-4c13-9cb1-4a12b09c05d1",
   "metadata": {},
   "source": [
    "Again everything looks good, except maybe for some small disagreement in the E/p variable, to be kept in mind for later. Note also the sharp cut on the total reconstructed momentum, even not applying our own cut. The reason is that the cut has already been applied in the pre-selection, and we are therefore unable to check the whole range in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c8ff64-df63-4d94-a3a7-e0abee14fd1d",
   "metadata": {},
   "source": [
    "## Measurement\n",
    "### Ke3 branching ratio measurement\n",
    "If we want now to perform the measurement of BR(Ke3) (normalized on K2pi), we can use the formula mentioned earlier and run the computation. As a reminder, we add it again here:\n",
    "$$\\text{BR(ke3)} = \\frac{N_\\text{ke3,sel}}{N_\\text{k2pi,sel}}\\cdot\\frac{A_\\text{k2pi}}{A_\\text{ke3}}\\cdot\\frac{\\varepsilon_\\text{trigg,k2pi}}{\\varepsilon_{trigg,ke3}}\\cdot\\text{BR(k2pi)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03dba3b-f8cd-4874-b7ba-a929ba25e22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "br_k2pi = constants.kaon_br_map[\"k2pi\"]\n",
    "epsilon_trigg_ratio = 1./1. # We decided to consider the trigger efficiency to be 100%\n",
    "ke3_acc = len(mc_ke3[\"ke3\"])/normalization_dict[\"ke3\"]\n",
    "acc_ratio = k2pi_acc / ke3_acc\n",
    "sel_ratio = len(data_ke3)/len(data_k2pi)\n",
    "br_ke3_meas = sel_ratio * acc_ratio * epsilon_trigg_ratio * br_k2pi\n",
    "print(f\"Measured Ke3 branching ratio: {br_ke3_meas:.2%}\")\n",
    "print(f\"Most precise Ke3 branching ratio (PDG): {constants.kaon_br_map['ke3']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a38eea-9853-4c38-b4f9-727d2fc59c4e",
   "metadata": {},
   "source": [
    "Congratulations! You have now measured the Ke3 branching fraction, and obtained a value that is very close to the best measurement. We have however not provided any uncertainties on that value yet. So this could be compatible by chance if we have very large uncertainties (e.g. of the order of 10%), or we could have a very precise measurement instead, still compatible if the uncertainties are of the order of 0.1%, or incompatible if our uncertainties are even smaller (e.g. 0.01%). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb338f7a-38d3-4558-a56f-28ba86554860",
   "metadata": {},
   "source": [
    "### Uncertainties estimates\n",
    "In summary, we cannot conclude anything if we do not measure the uncertainties.\n",
    "\n",
    "We could of course perform ourselves the task of computing all the uncertainties and propagating them to the end result manually. In python there is however a nice package called \"uncertainties\" which performs this in an automatic way. Although we have to be careful during complex operations as there may be additional subtelties not taken into account by the package. But we are here going to perform simple multiplication/division/addition operations without on simple numbers associated with uncertainties, and not correlated (although they are also taken care of automatically).\n",
    "\n",
    "So we can repeat all our previous operations, but adding uncertainties to our numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47d27ee-37fe-4400-a2e6-ba1658856a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uncertainties import ufloat\n",
    "# Let's make it a function as we will want to compute these numbers several more times later\n",
    "def compute_normalization(selected_data, selected_mc, br, mc_normalization):\n",
    "    n_mc_sel = selected_mc if isinstance(selected_mc, int) else len(selected_mc)\n",
    "    n_data_sel = selected_data if isinstance(selected_data, int) else len(selected_data)\n",
    "    \n",
    "    # Add poisson uncertainties \n",
    "    n_mc_sel = ufloat(n_mc_sel, np.sqrt(n_mc_sel), \"MC K2pi\")\n",
    "    n_data_sel = ufloat(n_data_sel, np.sqrt(n_data_sel), \"Data K2pi\")\n",
    "    \n",
    "    # No uncertainty on the normalization: we know exactly how many MC events were generated for this sample\n",
    "    acc = n_mc_sel/mc_normalization\n",
    "    n_K = n_data_sel/(acc * br * 1.0)\n",
    "\n",
    "    return acc, n_data_sel, n_K\n",
    "\n",
    "br_k2pi = ufloat(constants.kaon_br_map[\"k2pi\"], 0.0008, \"K2pi BR\")\n",
    "k2pi_acc, n_k2pi_data_sel, k2pi_N_K = compute_normalization(data_k2pi, mc_k2pi[\"k2pi\"], br_k2pi, normalization_dict[\"k2pi\"])\n",
    "print(f\"K2pi selection acceptance: {k2pi_acc:%}\")\n",
    "print(f\"Number of K2pi candidates in data: {n_k2pi_data_sel} candidates\")\n",
    "print(f\"Kaon flux: {k2pi_N_K:e} kaons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ae216e-c007-42b2-b2db-bff903dbb032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also make it a function as we will also repeat it a few times\n",
    "\n",
    "def compute_signal_br(selected_data, selected_mc, mc_normalization, flux):\n",
    "    n_mc_sel = selected_mc if isinstance(selected_mc, int) else len(selected_mc)\n",
    "    n_data_sel = selected_data if isinstance(selected_data, int) else len(selected_data)\n",
    "\n",
    "    # Add poisson uncertainties\n",
    "    n_mc_sel = ufloat(n_mc_sel, np.sqrt(n_mc_sel), \"MC Ke3\")\n",
    "    n_data_sel = ufloat(n_data_sel, np.sqrt(n_data_sel), \"Data Ke3\")\n",
    "\n",
    "    # Rather than computing each ratio separately, we use directly the flux which include all \n",
    "    # the relevant K2pi values (so we don't have to pass all the values individually to the function)\n",
    "    acc = n_mc_sel/mc_normalization\n",
    "    br_ke3_meas = n_data_sel / (acc * 1. * flux)\n",
    "\n",
    "    return acc, n_data_sel, br_ke3_meas\n",
    "\n",
    "ke3_acc, n_ke3_data_sel, br_ke3_meas = compute_signal_br(data_ke3, mc_ke3[\"ke3\"], normalization_dict[\"ke3\"], k2pi_N_K)\n",
    "br_ke3 = ufloat(constants.kaon_br_map['ke3'], 0.0004)\n",
    "print(f\"Measured Ke3 branching ratio: {br_ke3_meas:%}\")\n",
    "print(f\"Most precise Ke3 branching ratio (PDG): {br_ke3:%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3f249c-e824-4541-bb2b-22f2c4031ebb",
   "metadata": {},
   "source": [
    "We can now conclude: our measurement is slightly less precise that the most precise found in the PDG, but not by much (0.09% uncertainties vs. 0.04%), and they are perfectly compatible. However we have taken into account only statistical uncertainties, not any systematic uncertainties. As previously mentioned, we expect them to be small as they will cancel out in the ratiom but still they should be taken into account and could possibly be of a similar magnitude to the statistical uncertainties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efff9a1f-562c-412d-86b8-de94e1aab903",
   "metadata": {},
   "source": [
    "### Systematic uncertainties\n",
    "Systematic uncertainties are linked to possible bias that could be introduced during the selection/analysis process. \n",
    "While checking the data/MC agreement in the distribution on which we introduce cuts in the previous section, two of them (the CDA and the electron E/p) had some level of disagreement and we mentioned that we should keep those in mind. It is now the time to see if those would introduce possible bias on the final result. The possible bias that could be introduced will depend on the threshold we choose for the cut and will result in the final measurement changing (slightly or more) depending on which value we choose. The way to check for systematic bias and assign uncertainties is therefore to perform the selection while varying the cut value around the nominal one.\n",
    "\n",
    "Technically we will achieve this by removing the original cut from the selection, and repeat the whole selection-measurement process introducing the cut with a new value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e1fd5-900f-41b4-b6b8-8a8d4a3473d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "# Let's start with a few useful functions\n",
    "\n",
    "# This function will run the process described above:\n",
    "#  - Take as input the normalization and signal selections\n",
    "#  - Which cut to remove and what replacements we will use (and if the change must happen on both normalization and signal selections, or only one of them\n",
    "#  - Some other variables that are needed by the compute_normalization and compute_signal_br functions\n",
    "# Then it will run over all the alternative cuts, recording all the useful final numbers in an array, for each tested alternative\n",
    "def systematic_checks(normalization_selection: List[Callable], signal_selection: List[Callable], \n",
    "                      test_cut: Union[Callable, str], replacement_cuts: List[Callable], \n",
    "                      br_norm: float, norm_name: str, signal_name: str, truncate_which: str = \"both\") -> Tuple[float]:\n",
    "\n",
    "    # Truncate the selections if necessary\n",
    "    truncated_norm_selection = hlf.remove_cut(normalization_selection, test_cut) if truncate_which in [\"both\", \"norm\"] else normalization_selection\n",
    "    truncated_signal_selection = hlf.remove_cut(signal_selection, test_cut) if truncate_which in [\"both\", \"signal\"] else signal_selectionl\n",
    "\n",
    "    # Prepare lists for output\n",
    "    flux_list = []\n",
    "    n_norm_list = []\n",
    "    acc_norm_list = []\n",
    "    n_sig_list = []\n",
    "    acc_sig_list = []\n",
    "    br_sig_list = []\n",
    "\n",
    "    # Loop over the alternative cuts\n",
    "    for new_cond in tqdm(replacement_cuts):\n",
    "        # Perform the alternative selection\n",
    "        norm_selection = truncated_norm_selection + [new_cond] if truncate_which in [\"both\", \"norm\"] else truncated_norm_selection\n",
    "        signal_selection = truncated_signal_selection + [new_cond] if truncate_which in [\"both\", \"signal\"] else truncated_signal_selection\n",
    "        data_test_norm, mc_test_norm = hlf.select_all(data_orig, mc_dict_orig, norm_selection)\n",
    "        data_test_signal, mc_test_signal = hlf.select_all(data_orig, mc_dict_orig, signal_selection)\n",
    "\n",
    "        # Compute the final numbers\n",
    "        norm_acc, n_norm_data_sel, flux = compute_normalization(data_test_norm, mc_test_norm[norm_name], br_norm, normalization_dict[norm_name])\n",
    "        sig_acc, n_sig_data_sel, br_sig_meas = compute_signal_br(data_test_signal, mc_test_signal[signal_name], normalization_dict[signal_name], flux)\n",
    "\n",
    "        # Populate the output lists\n",
    "        flux_list.append(flux)\n",
    "        n_norm_list.append(n_norm_data_sel)\n",
    "        acc_norm_list.append(norm_acc)\n",
    "        n_sig_list.append(n_sig_data_sel)\n",
    "        acc_sig_list.append(sig_acc)\n",
    "        br_sig_list.append(br_sig_meas)\n",
    "\n",
    "    return flux_list, n_norm_list, acc_norm_list, n_sig_list, acc_sig_list, br_sig_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f7c07d-3885-46ee-86b8-77c3d4270166",
   "metadata": {},
   "source": [
    "Now perform the systematic checks (each will take a bit of time as we have to run the whole selections (4) N times).\n",
    "\n",
    "First for the CDA. The nominal value was 25, so we can test from 15 to 40 in steps of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8685fbd-4d1c-40d0-bbb1-ab33b20a8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "cda_values = list(np.arange(15, 40+5, 5))\n",
    "replacement_cuts = [hlf.make_cda_cut(None, cda_value) for cda_value in cda_values]\n",
    "flux_list, n_k2pi_list, acc_k2pi_list, n_ke3_list, acc_ke3_list, br_ke3_list = systematic_checks(common_1track_2cluster_selection + k2pi_selection, \n",
    "                                                                                                  common_1track_2cluster_selection + ke3_selection, \n",
    "                                                                                                  cda_cond, replacement_cuts, \n",
    "                                                                                                  br_k2pi, \"k2pi\", \"ke3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ef7b0-4ed6-47a2-881b-17864791dd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(21, 10))\n",
    "histo.plot_variations(cda_values, flux_list, 2, \"$N_{K,\\mathrm{k2pi}}$\", \"CDA\", ax[0][0])\n",
    "histo.plot_variations(cda_values, n_k2pi_list, 2, \"$N_\\mathrm{k2pi}$\", \"CDA\", ax[0][1])\n",
    "histo.plot_variations(cda_values, acc_k2pi_list, 2, \"$A_\\mathrm{k2pi}$\", \"CDA\", ax[0][2])\n",
    "\n",
    "histo.plot_variations(cda_values, n_ke3_list, 2, \"$N_\\mathrm{ke3}$\", \"CDA\", ax[1][0])\n",
    "histo.plot_variations(cda_values, acc_ke3_list, 2, \"$A_\\mathrm{ke3}$\", \"CDA\", ax[1][1])\n",
    "histo.plot_variations(cda_values, br_ke3_list, 2, \"BR(ke3)$\", \"CDA\", ax[1][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb47ba3-b230-4435-af5b-2a640a780806",
   "metadata": {},
   "source": [
    "We see here that varying the cut has essentially no influence at all on the final result. The changes are very small, much smaller than the statistical uncertainties.\n",
    "\n",
    "Now for the cut on the E/p for the electron PID. This cut applies only to the signal selection (for the Ke3 electron) but is not present for the normalization K2pi selection (we do have a E/p cut for the pion PID, but it is a different cut and we did not observe and data/MC discrepancy in that distribution). The E/p cut has two values, lower and upper cuts. We will perform the variation on one edge at a time. First the lower edge where the nominal value was 0.95, so we will vary it from 0.9 to 0.97 in steps of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8907f084-43f9-49a4-8278-9dff368eb48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eop_values = list(np.arange(0.90, 0.97+0.01, 0.01))\n",
    "replacement_cuts = [hlf.make_eop_cut(eop_value, 1.05, which_track=\"track1\") for eop_value in eop_values]\n",
    "flux_list, n_k2pi_list, acc_k2pi_list, n_ke3_list, acc_ke3_list, br_ke3_list = systematic_checks(common_1track_2cluster_selection + k2pi_selection, \n",
    "                                                                                                  common_1track_2cluster_selection + ke3_selection, \n",
    "                                                                                                  ke3_selection[0], replacement_cuts, br_k2pi, \"k2pi\", \"ke3\",\n",
    "                                                                                                  truncate_which=\"signal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3d07c0-f36e-4342-96ed-81b7feb57806",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(21, 10))\n",
    "\n",
    "histo.plot_variations(eop_values, flux_list, 5, \"$N_{K,\\mathrm{k2pi}}$\", \"Electron E/p lower cut\", ax[0][0])\n",
    "histo.plot_variations(eop_values, n_k2pi_list, 5, \"$N_\\mathrm{k2pi}$\", \"Electron E/p lower cut\", ax[0][1])\n",
    "histo.plot_variations(eop_values, acc_k2pi_list, 5, \"$A_\\mathrm{k2pi}$\", \"Electron E/p lower cut\", ax[0][2])\n",
    "\n",
    "histo.plot_variations(eop_values, n_ke3_list, 5, \"$N_\\mathrm{ke3}$\", \"Electron E/p lower cut\", ax[1][0])\n",
    "histo.plot_variations(eop_values, acc_ke3_list, 5, \"$A_\\mathrm{ke3}$\", \"Electron E/p lower cut\", ax[1][1])\n",
    "histo.plot_variations(eop_values, br_ke3_list, 5, \"BR(ke3)$\", \"Electron E/p lower cut\", ax[1][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8ab6c9-c391-406e-8b2a-72acf6b475e5",
   "metadata": {},
   "source": [
    "As expected we see no variation in the normalization histogram (as the cut is not present in the normalization selection). Contrary to the previous check, we do see a variation in the signal measurements. In particular the BR(ke3) is changing significantly once the cut value goes closer to 1. This is expected as we are starting to cut quite hard into the actual electron sample above 0.95, and we know that such high cuts are not reasonable. Setting this point aside, we see that even though we see variations, they remain within the statistical uncertainties.\n",
    "\n",
    "Then for the upper edge with a nominal value of 1.05, we will vary from 1.03 to 1.10 in steps of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc63c5b-0488-46fa-8282-fa62c8776f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "eop_values = list(np.arange(1.03, 1.10+0.01, 0.01))\n",
    "replacement_cuts = [hlf.make_eop_cut(0.95, eop_value, which_track=\"track1\") for eop_value in eop_values]\n",
    "flux_list, n_k2pi_list, acc_k2pi_list, n_ke3_list, acc_ke3_list, br_ke3_list = systematic_checks(common_1track_2cluster_selection + k2pi_selection, \n",
    "                                                                                                  common_1track_2cluster_selection + ke3_selection, \n",
    "                                                                                                  ke3_selection[0], replacement_cuts, br_k2pi, \"k2pi\", \"ke3\",\n",
    "                                                                                                  truncate_which=\"signal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e808cf-1f24-4866-92e7-d64288bb1895",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(21, 10))\n",
    "\n",
    "histo.plot_variations(eop_values, flux_list, 2, \"$N_{K,\\mathrm{k2pi}}$\", \"Electron E/p upper cut\", ax[0][0])\n",
    "histo.plot_variations(eop_values, n_k2pi_list, 2, \"$N_\\mathrm{k2pi}$\", \"Electron E/p upper cut\", ax[0][1])\n",
    "histo.plot_variations(eop_values, acc_k2pi_list, 2, \"$A_\\mathrm{k2pi}$\", \"Electron E/p upper cut\", ax[0][2])\n",
    "\n",
    "histo.plot_variations(eop_values, n_ke3_list, 2, \"$N_\\mathrm{ke3}$\", \"Electron E/p upper cut\", ax[1][0])\n",
    "histo.plot_variations(eop_values, acc_ke3_list, 2, \"$A_\\mathrm{ke3}$\", \"Electron E/p upper cut\", ax[1][1])\n",
    "histo.plot_variations(eop_values, br_ke3_list, 2, \"BR(ke3)$\", \"Electron E/p upper cut\", ax[1][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11bf944-c5a2-4ceb-9629-5bc8877739f3",
   "metadata": {},
   "source": [
    "Similarly to the lower cut, we see variations in the upper cut on the signal values. We can also do a similar assessment as previously concerning the point at 1.03, which is too close to E/p=1.0. However in this case we also see variations that are larger than the statistical uncertainties when moving away from 1, and seem to stabilize when going far enough. We can assigned a systematic uncertainty of the size of the largest variation (again excluding those that we know are unreasonable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c808840b-0969-4bc5-a0c0-905e17a4999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore the first entry corresponding to the too tight cut\n",
    "syst_br_list = br_ke3_list[1:]\n",
    "# Compute the difference with the nominal value\n",
    "diff = np.array(syst_br_list) - br_ke3_meas\n",
    "# Take the largest difference\n",
    "syst_error = max(diff).n\n",
    "\n",
    "br_ke3_meas_wsyst = br_ke3_meas + ufloat(0, syst_error, \"Systematic\")\n",
    "print(f\"Systematic uncertainty assigned to the E/p cut: {syst_error:.0e}\")\n",
    "print(f\"Final measurement with systematic uncertainties: {br_ke3_meas_wsyst:%}\")\n",
    "print(f\"Most precise Ke3 branching ratio (PDG): {br_ke3:%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926bec8d-e8fd-4f7c-9c16-daad449c516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the various contributions to the uncertainties\n",
    "br_ke3_meas_wsyst.error_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9fc82b-f387-4b50-91dc-93651fcbfaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print them in order of importance, with a more convenient formatting\n",
    "print(\"\\n\".join([f\"{uname.tag:<10}: {uvalue:.0e}\" for uname, uvalue in sorted(br_ke3_meas_wsyst.error_components().items(), key=lambda item: item[1], reverse=True)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102cf663-16b7-4013-9878-770f3a267471",
   "metadata": {},
   "source": [
    "We have added a small systematic uncertainty, but our measurement is still quite precise. Congratulations!\n",
    "\n",
    "Our measurement being dominated by statistical uncertainties from the data, we only need to increase the statistics to improve it. However, note that the systematic uncertainty is the second largest at $5\\times10^{-4}$, largers than the total uncertainty on the best measurement available in the PDG. This means that if we want to improve the measurement to a level better than the PDG, we would have also to reduce the systematic uncertainties, not only the statistical ones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
