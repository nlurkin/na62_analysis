{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed54cb3-33bf-474a-a3f9-64694bf2f1e9",
   "metadata": {},
   "source": [
    "# Event selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cb673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets first import all we need\n",
    "import uproot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from functools import partial\n",
    "from na62 import prepare, hlf, extract, constants, stats, histo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540c36bd-f340-4f6d-a715-42d589363496",
   "metadata": {},
   "source": [
    "## Data processing: from raw data to high-level objects\n",
    "The NA62 data, after being acquired, are first reconstructed. This means we take our raw (binary) data and we extract for each event the signals recorded for each detector individually. For each detectors, these signals are reconstructed as hits containing information like time, position, and geometrical channel ID (i.e. related to physical position rather than electronics position). Hits are further grouped together to reconstruct and form candidates: candidates are more complex objects generally meant to relate to a single particle interacting with the detector. These candidates can be referred to with other names for certain detectors: LKr -> cluster, Spectrometer -> track, GTK -> beam track.\n",
    "\n",
    "At the analysis level, candidates from multiple detectors can be associated together in space (geometrically) and in time to form higher level objects:\n",
    " - beam kaon: GTK track associated to a KTAG candidate\n",
    " - downstream track: spectrometer track associated to eventual candidates from CHOD, NewCHOD, MUV1-3, LKr, RICH\n",
    " - vertex: association between GTK and spectrometer tracks, or associated between spectrometer tracks only, or association between LKr clusters (neutral vertex)\n",
    "\n",
    "## Pile-up\n",
    "As you may have guessed, the data presented to you here consist of events in the form of association of several of these high-level objects. However to reach that point, there has been already a sizable amount of selection performed at analysis level to make these associations. In fact one single event will usually consist in many more of those high-level objects coming from pile-up. Pile-up refers to valid events (either a beam kaon decay, a beam pion decay, a single beam muon) which are *not* (with caveat) related to the event that generated the trigger. In order to make sure we have **all** the information relating to that specific events, we are requesting signals from detector that are spread within ~100 ns of the trigger time. This means we have a lot of pile-up in each event, which is spread in time randomly within the event. But by applying timing cuts at the analysis level, we can easily discard a very large fraction of those pile-up events. There is still a small fraction of events remaining which happen to be at the same time as the triggered event. As it is not possible to distinguish it from the triggered event, additional selection criterion must be applied to ensure it is a valid kaon decay. As the pile-up event is at the same time as the kaon decay, we usually have several possibilities to combine the high-level objects. For instance, if you have one track and 3 isolated clusters and looking for a K2pi decay, you have three association possibilities for the clusters: combine clusters 1 and 2 to form a pi0, combines cluster 2 and 3, or combines clusters 1 and 3. This is called the combinatorics and you can apply conditions on each of all the possible options. If you manage to find a single option that satisfies all your requirements, this could be the correct one and you go ahead forming an event with this selection of high-level object. If not you usually reject the event.\n",
    "\n",
    "This process of eliminating the pile-up has already been done for you in the data that are available here, and this is the reason why each of your event has only one to three tracks and zero to two clusters. There remains one category of pile-up contaminating your sample (the caveat mentionned above): events that are **not** a kaon decay, but a combination of independent particles that look like a kaon decay. These required additional selection criterion, some of those have already been applied and some that we can apply ourselves in these data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31da7f08",
   "metadata": {},
   "source": [
    "## Pre-selection\n",
    "As mentionned above, a comprehensive pre-selection has already been applied to the data available here. You will find below the details on the criterion that have been used for each of the selected 'event_type'.\n",
    "\n",
    "### K3pi selection\n",
    " - Control trigger\n",
    " - Exactly one three-track vertex within 6 ns of the trigger time\n",
    " - Each track must be in the acceptance of NewCHOD and 4 spectrometer chambers\n",
    " - Tracks must be further than 10 cm away from each other at Straw1\n",
    " - $q_\\text{vtx} = +1$\n",
    " - Vertex between 104 m and 180 m and $\\chi^2<25$\n",
    " - Reconstructed total momentum $p_\\text{tot}$ must be within 3 GeV of 75 GeV\n",
    " - Reconstructed three-pion invariant mass $m_{3\\pi}$ must between 490 MeV and 497 MeV\n",
    "\n",
    "### Kmu2 selection\n",
    " - Control trigger\n",
    " - Less than 10 tracks\n",
    " - Exactly one good track:\n",
    "     - Must have CHOD or NewCHOD associated signal\n",
    "     - Within 10 ns from the trigger time\n",
    "     - Signal in 4 spectrometer chambers\n",
    "     - Track $\\chi^2<20$\n",
    "     - Not forming a vertex with any other track. Vertex if:\n",
    "         - cda < 50 mm\n",
    "         - z vertex between 60 m and 200 m\n",
    " - $q=+1$\n",
    " - Track momentum betwen 5 GeV and 70 GeV\n",
    " - Track in geometric acceptance of all 4 spectrometer stations, MUV3\n",
    " - Vertex between 120 m and 180 m with CDA<40 mm\n",
    " - Closest KTAG candidate in time must be within 2 ns of the track CHOD time, or 5 ns from the track NewCHOD time if CHOD time is not available\n",
    " - No in-time activity in the LAV, IRC and SAC\n",
    " - Track must have MUV3 signal associated to the track within 1.5 ns of the KTAG time, and within 2 ns of the track CHOD time (5 ns of NewCHOD if CHOD not available)\n",
    " - Track $E/p < 0.2$\n",
    " - Missing mass squared (muon hypothesis) $m_\\text{miss}^2(\\mu) < 0.02~\\text{GeV}^2$\n",
    "\n",
    "### K2pi selection\n",
    " - control trigger\n",
    " - At least 1 track and 3 clusters\n",
    " - At least 2 good clusters:\n",
    "     - Within 20 ns of the trigger time\n",
    "     - Further than 20 mm from a dead cell\n",
    "     - Cluster must be electromagnetic\n",
    "     - At least 1 GeV\n",
    "     - Not associated to a Spectrometer track\n",
    "     - Must be isolated\n",
    " - Build pairs of clusters as pi0:\n",
    "   - Within 5 ns of each other\n",
    "   - Further than 200 mm apart\n",
    "   - Energy sum between 2 GeV and 75 GeV\n",
    " - Exactly 1 good track definition:\n",
    "     - At least one pi0 within 2 ns of the track\n",
    "     - $q=+1$\n",
    "     - Not fake\n",
    "     - Less than 20 GeV between raw momentum and fitted momentum\n",
    "     - Momentum between 5 GeV and 70 GeV\n",
    "     - Vertex wrt. beam axis must be between 105 m and 180 m\n",
    "     - CDA < 30 mm\n",
    "     - Inside the geometric acceptance of NewCHOD, all 4 Spectrometer stations, LKr\n",
    " - Select closest pi0 in time\n",
    " - No MUV3 signal associated to the track\n",
    " - Must have LKr cluster associated to the track, and $E/p < 0.9$\n",
    " - No in-time activity in the LAV, IRC and SAC\n",
    " - Reconstructed kaon mass $m_{\\pi\\gamma\\gamma}$ between 460 MeV and 520 MeV\n",
    " - Missing mass squared (pion hypothesis) $m_\\text{miss}^2(\\pi) < 0.015~\\text{GeV}^2$\n",
    "\n",
    "### Kmu3 selection\n",
    " - Control trigger\n",
    " - At least one track\n",
    " - Exactly one good track:\n",
    "     - Must have MUV3 association\n",
    "     - Vertex wrt. beam axis must be between 110 m and 180 m\n",
    "     - CDA < 25 mm\n",
    "     - Momentum between 5 GeV and 50 GeV\n",
    "     - $q = +1$\n",
    "     - Track $\\chi^2 < 20$\n",
    "     - In acceptance of NewCHOD, 4 spectrometer chambers, LKr and MUV3\n",
    " - No other track within 10 ns of the good track\n",
    " - Must have exactly two good LKr clusters:\n",
    "     - Energy > 2 GeV\n",
    "     - Within 6 ns of the track\n",
    "     - Further than 150 mm from the track impact point on LKr\n",
    " - Neutral vertex within 10 m of charged vertex\n",
    " - No in-time activity in the LAV, IRC and SAC\n",
    " - Reconstructed total momentum $p_\\text{tot}$ must be between 15 GeV and 70 GeV\n",
    " - Reconstructed transverse total momentum $p_{\\text{tot},T}$ must be between 40 and 250 MeV\n",
    " - Missing mass squared (muon hypothesis) $m_\\text{miss}^2(\\mu) < 0.01~\\text{GeV}^2$\n",
    "\n",
    "### Ke3 selection\n",
    " - Control trigger\n",
    " - At least one track\n",
    " - Exactly one good track:\n",
    "     - Must not have MUV3 association\n",
    "     - Vertex wrt. beam axis must be between 110 m and 180 m\n",
    "     - CDA < 25 mm\n",
    "     - Momentum between 5 GeV and 50 GeV\n",
    "     - $q = +1$\n",
    "     - Track $\\chi^2 < 20$\n",
    "     - In acceptance of NewCHOD, 4 spectrometer chambers, LKr and MUV3\n",
    " - No other track within 10 ns of the good track\n",
    " - Must have exactly two good LKr clusters:\n",
    "     - Energy > 2 GeV\n",
    "     - Within 6 ns of the track\n",
    "     - Further than 150 mm from the track impact point on LKr\n",
    " - Neutral vertex within 10 m of charged vertex\n",
    " - No in-time activity in the LAV, IRC and SAC\n",
    " - Reconstructed total momentum $p_\\text{tot}$ must be between 15 GeV and 70 GeV\n",
    " - Reconstructed transverse total momentum $p_{\\text{tot},T}$ must be between 40 and 250 MeV\n",
    " - Missing mass squared (electron hypothesis) $m_\\text{miss}^2(e) < 0.01~\\text{GeV}^2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24afc8b7-e820-485d-8ac3-648233678855",
   "metadata": {},
   "source": [
    "## Criterions categories\n",
    "From the selections above we can roughly create some \"categories\" of criterion:\n",
    " - Trigger\n",
    " - Basic objects requirements\n",
    " - Object selection conditions\n",
    "     - Timing\n",
    "     - Track quality\n",
    "     - Charge\n",
    "     - Momentum\n",
    "     - Vertex\n",
    "     - Geometric acceptance\n",
    " - Additional requirements\n",
    " - Pile-up veto\n",
    " - Reconstructed quantities\n",
    "\n",
    "This can be useful to roughly compare the selections and identify differences. The table below summarises the selection according to these categories\n",
    "\n",
    "| Type | Sample | Trigger | Basic requirements | O. timing | O. quality | O. charge | O. momentum | O. vertex | O. Geo. Acc. | Add. Req. | Veto | Reco quantities |\n",
    "| --:    | --:    | :--:    | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n",
    "| 3-track | K3pi   | Control | One 3-track vertex | $|t_\\text{vtx} - t_\\text{trigger}| < 6~\\text{ns}$ | $d_{i,j} > 10~\\text{cm}$ | +1 | N/A | $104~m < Z_\\text{vtx} < 180~m$ & $\\chi^2<25$ | NewCHOD & 4 Straw | N/A | N/A | $|p_\\text{tot} - 75~\\text{GeV}|<3~\\text{GeV}$ & $490~\\text{MeV} < m_{3\\pi} < 497~\\text{MeV}$ |\n",
    "| 1-track | Kmu2   | Control | < 10 tracks | $|t_\\text{track} - t_\\text{trigger}| < 10~\\text{ns}$ | $\\chi^2 < 20$ & No other vertex | +1 | $5~\\text{GeV} < p < 70~\\text{GeV}$ | $120~m < Z_\\text{vtx} < 180~m$ & $\\text{CDA} < 40~mm$ | MUV3 & 4 Straw | Signal in 4 Straw and (CHOD or NewCHOD) & KTAG in-time & MUV3 in-time signal & $E/p < 0.2$ | LAV, IRC, SAC | $m_\\text{miss}^2(\\mu) < 0.02~\\text{GeV}^2$ |\n",
    "| 1-track & 2-clusters | K2pi   | Control | 1 track & 3 clusters | $|t_\\text{track}-t_{\\pi_0}| < 2~\\text{ns}$; Cluster: $|t_\\text{cluster} - t_\\text{trigger}|<20~\\text{ns}$ & $\\Delta t_{i,j}<5~\\text{ns}$ | Track: Not fake & $\\Delta p_\\text{raw,fit}<20~\\text{GeV}$; Cluster: $\\Delta d_\\text{dead-cell} > 20~\\text{mm}$ & Electromagnetic & Not associated to track & Isolated & $\\Delta d_{i,j} > 200~\\text{mm}$ | Track: +1 | Track: $5~\\text{GeV} < p < 70~\\text{GeV}$; Cluster: $E>1~\\text{GeV}$ & $2~\\text{GeV} < E_1 + E_2 < 75~\\text{GeV}$ | $105~m < Z_\\text{vtx} < 180~m$ & $\\text{CDA} < 30~mm$ | NewCHOD & 4 Straw | Track: no MUV3 &  $E/p<0.9$ | LAV, IRC, SAC | $460~\\text{MeV} < m_K < 520~\\text{MeV}$ & $m_\\text{miss}^2(\\pi) < 0.015~\\text{GeV}^2$ |\n",
    "| 1-track & 2-clusters | Kmu3 | Control | N/A | $|t_\\text{cluster} - t_\\text{track}| < 6~\\text{ns}$ | Track: $\\chi^2 < 20$ & MUV3 association; Cluster: $\\Delta d_\\text{cls,track} > 150~\\text{mm}$ | Track: +1 | Track: $5~\\text{GeV} < p < 50~\\text{GeV}$; Cluster: $E>2~\\text{GeV}$ | Track: $110~m < Z_\\text{ch. vtx} < 180~m$ & $\\text{CDA} < 25$; Cluster: $|Z_\\text{ch. vtx} - Z_\\text{neut. vtx}|<10~m$ | Track: NewCHOD & 4 Straw & LKr & MUV3 | No track within 10 ns | LAV, IRC, SAC | $15~\\text{GeV} < p_\\text{tot} < 70~\\text{GeV}$ & $40~\\text{MeV} < p_{\\text{tot},T} < 250~\\text{MeV}$ & $m_\\text{miss}^2(\\mu) < 0.01~\\text{GeV}^2$ |\n",
    "| 1-track & 2-clusters | Ke3 | Control | N/A | $|t_\\text{cluster} - t_\\text{track}| < 6~\\text{ns}$ | Track: $\\chi^2 < 20$ & No MUV3 association; Cluster: $\\Delta d_\\text{cls,track} > 150~\\text{mm}$ | Track: +1 | Track: $5~\\text{GeV} < p < 50~\\text{GeV}$; Cluster: $E>2~\\text{GeV}$ | Track: $110~m < Z_\\text{ch. vtx} < 180~m$ & $\\text{CDA} < 25$; Cluster: $|Z_\\text{ch. vtx} - Z_\\text{neut. vtx}|<10~m$ | Track: NewCHOD & 4 Straw & LKr & MUV3 | No track within 10 ns | LAV, IRC, SAC | $15~\\text{GeV} < p_\\text{tot} < 70~\\text{GeV}$ & $40~\\text{MeV} < p_{\\text{tot},T} < 250~\\text{MeV}$ & $m_\\text{miss}^2(e) < 0.01~\\text{GeV}^2$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f5f502-d990-4549-88b7-7024541bc75c",
   "metadata": {},
   "source": [
    "Even though all the selections seem to contain a similar set of conditions, the cut values are sometimes differing slightly. As we will eventually compare events type selected with different selections, it is best to harmonize the cut values. This will avoid introducing systematic uncertainties due to different behaviour and different agreement between MC and data in different ranges of the variables on which we introduce conditions.\n",
    "\n",
    "The conditions where we can see obvious differences that can be harmonized are in the following list. When common criterion are selected, we have to use the most constraining values as we cannot \"undo\" what was already applied before presenting these data.\n",
    " - $Z \\text{vtx}$: common range is between 120 m and 180 m\n",
    " - LKr distance between track and cluster: common limit of at least 150 mm\n",
    " - LKr distance between clusters: common limit of at least 200 mm\n",
    " - Cluster Energy: common limit of at least 2 GeV\n",
    " - Vertex CDA: common limit of less than 25 mm\n",
    " - Distance between charged and neutral vertices: common limit of less than 10 m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c27608-6aa2-4a29-b9db-fe09b7127e68",
   "metadata": {},
   "source": [
    "## Technical aside \n",
    "### Functional cuts\n",
    "As you have seen, we are working in this project with pandas Dataframes. Among other things the dataframe allow us to easily implement cuts, which you have experienced already in the previous notebooks. The way we have done this so far is an operation done in multiple steps (example selecting a track momentum range):\n",
    " - Identify all events satisfying the lower range: `low_cond = df[\"track1_momentum_mag\"]>15000`\n",
    " - Identify all events satisfying the upper range: `upper_cond = df[\"track1_momentum_mag\"]<40000`\n",
    " - Combine both criterion: `cond = low_cond & upper_cond`\n",
    " - Select events satisfying both conditions: `df.loc[cond]`\n",
    "\n",
    "The first three steps actually creates a boolean array with one entry for each event: True or False. Only the last step actually filter out the events we are not interested in. We often shortened this in a single line:\n",
    "```python\n",
    "df.loc[(df[\"track1_momentum_mag\"]>15000) & (df[\"track1_momentum_mag\"]<40000)]\n",
    "df.loc[(df[\"track2_momentum_mag\"]>15000) & (df[\"track2_momentum_mag\"]<40000)]\n",
    "df.loc[(df[\"track3_momentum_mag\"]>15000) & (df[\"track3_momentum_mag\"]<40000)]\n",
    "```\n",
    "\n",
    "However we find that these lines are a bit verbose with a lot of redundancy: `df` is present three times, `track1_momentum_mag` twice. It also lacks flexibility: if we want to apply this condition on all three tracks, we have to write the line three times, with two small changes. Also if we want to try a different value of pmin and pmax, we may have to change this in many places, risking to forget changing it in one place. This could be improved by writing functions and loops, but the code would loose readability. When you read a selection code, you do not care how you applied a momentum cut to three tracks, only that you did and which cut values were used.\n",
    "\n",
    "Instead we can take advantage of one functionality provided by pandas: the argument of the `.loc()` function can be a `Callable` (i.e. a function or a lambda) taking a single argument (the dataframe) and returning a boolean array used for the filtering. We can therefore use it like this:\n",
    "```python\n",
    "def t1_momentum_cut_15_40(df):\n",
    "    return (df[\"track1_momentum_mag\"]>15000) & (df[\"track1_momentum_mag\"]<40000)\n",
    "def t2_momentum_cut_15_40(df):\n",
    "    return (df[\"track2_momentum_mag\"]>15000) & (df[\"track2_momentum_mag\"]<40000)\n",
    "def t3_momentum_cut_15_40(df):\n",
    "    return (df[\"track3_momentum_mag\"]>15000) & (df[\"track3_momentum_mag\"]<40000)\n",
    "\n",
    "df.loc[t1_momentum_cut_15_40]\n",
    "df.loc[t2_momentum_cut_15_40]\n",
    "df.loc[t3_momentum_cut_15_40]\n",
    "```\n",
    "\n",
    "This is so far not much of an improvement, except for the case where we use these conditions in multiple places. In that case we have to change only one function per track. We can another improvement by using another python propertie: we can write function that create functions!\n",
    "```python\n",
    "def make_momentum_cut(min_p, max_p, which_object = None):\n",
    "    # We can do some pre-processing\n",
    "    if which_object is None:\n",
    "        which_object = \"\"\n",
    "    else:\n",
    "        which_object = f\"{which_object}_\"\n",
    "\n",
    "    # We define the function from the example above\n",
    "    def cut(df):\n",
    "        # This function also works if we provide directly the series!\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            serie_cut = df[f\"{which_object}momentum_mag\"]\n",
    "        else:\n",
    "            serie_cut = df\n",
    "        # We implement the case where we don't have a lower or an upper limit\n",
    "        min_momentum_range = serie_cut > min_p if min_p else True\n",
    "        max_momentum_range = serie_cut < max_p if max_p else True\n",
    "        return min_momentum_range & max_momentum_range\n",
    "    # Return the function we created\n",
    "    return cut\n",
    "\n",
    "from functools import partial\n",
    "# Create a new function that binds the min/max parameters, but not which_object\n",
    "momentum_cut_15_40 = partial(make_momentum_cut, 15000, 40000)\n",
    "\n",
    "df.loc[momentum_cut_15_40(\"track1\")]\n",
    "df.loc[momentum_cut_15_40(\"track2\")]\n",
    "df.loc[momentum_cut_15_40(\"track3\")]\n",
    "```\n",
    "\n",
    "True, the overal code is longer, but the on the other hand the interesting part (that last 4 lines) are now much easier to read (reading the line we understand immediately what condition we apply), less error prone (we have a single place to change if we want to modify the cut everywhere), and more flexible (with the same function we can apply to any track without redefinition, and we can easily define another cut in a single line). The `make_momentum_cut` function is implemented in na62.hlf taking even further advantage of those principles by leveraging a generic *min_max* cut function. It is now very short:\n",
    "```python\n",
    "def make_momentum_cut(min_p, max_p, which_object = None) -> Callable:\n",
    "    return make_min_max_cut(min_p, max_p, which_value=\"momentum_mag\", which_object=which_object)\n",
    "```\n",
    "\n",
    "Many \"make_*_cut\" condition functions have been defined in such a way in the na62.hlf module. Have a look at the documentation to see which are available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b04ad9c-b7df-42c4-a8b0-2e1cd814171a",
   "metadata": {},
   "source": [
    "### Data/MC histograms\n",
    "Because we now have MC samples, what we will want to do is to compare our data with the simulation. This is done by plotting together the data and the MC on a single plot. There are however a few things to take into accout:\n",
    " - The data comprise the sum of all channels, therefore the MC contributions should be stacked (plotted on **top** of each other and not besides each other).\n",
    " - The MC events available for each channel do not represent the same number of kaons. Three parameters have to be taken into account to be able to normalize them relative to each other:\n",
    "   - The \"norm\" parameters retrieved in the file. It is the total number of events that were generated (how many kaon decays). Using these we can take into account their relative size (in terms of generation).\n",
    "   - The branching fraction of each sample.\n",
    "   - The selection acceptance/efficiency of each sample. The selection cuts do not have the same effect on each sample (because their kinematic distributions are different). \n",
    " - Once the MC samples are correctly normalized relative to each other, the total integrated MC sample must also be normalized to correspond to the number of data events that we have collected.\n",
    "\n",
    "**Exercise**: Imagine that you simulate 1000 kmu2 events out of which 500 pass your selection, and 2000 K3pi events out of which 100 pass your selection. How many kaons does that corresponds too for each sample? What should be their relative weights? If we select a data sample corresponding to 10000 events (consisting only of kmu2 and k3pi - your selection is very pure), how many kmu2 and k3pi does that correspond to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616dd82e-b629-4fbb-8ea9-85ea1355c5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_kmu2 = 1000\n",
    "n_k3pi = 2000\n",
    "n_kmu2_sel = 500\n",
    "n_k3pi_sel = 100\n",
    "ndata_sel = 10000\n",
    "\n",
    "# First compute to number of kaons that correspond to those simulated events\n",
    "n_kmu2_kaons = n_kmu2 / constants.kaon_br_map[\"kmu2\"]\n",
    "n_k3pi_kaons = n_k3pi / constants.kaon_br_map[\"kmu2\"]\n",
    "\n",
    "print(f\"Number of Kaons for kmu2: {n_kmu2_kaons:.2f}\")\n",
    "print(f\"Number of Kaons for kmu2: {n_k3pi_kaons:.2f}\")\n",
    "print(f\"Relative weight: {n_k3pi_kaons/n_kmu2_kaons:.2f} (1 k3pi kaon is worth {n_k3pi_kaons/n_kmu2_kaons:.0f} kmu2 kaons)\")\n",
    "\n",
    "# Now compute the weights for each selected event\n",
    "w_k3pi = (n_k3pi_sel/n_k3pi_kaons)\n",
    "w_kmu2 = (n_kmu2_sel/n_kmu2_kaons)\n",
    "print(f\"Relative selected weight: {w_k3pi/w_kmu2:.2f} (1 selected k3pi kaon is worth {w_k3pi/w_kmu2:.0f} selected kmu2 kaons)\")\n",
    "\n",
    "mc_normalization = w_k3pi + w_kmu2\n",
    "\n",
    "print(f\"Number of Kmu2 in the data: {ndata_sel*w_kmu2/mc_normalization:.0f}\")\n",
    "print(f\"Number of K3pi in the data: {ndata_sel*w_k3pi/mc_normalization:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa94f36e-cdb8-4c36-a7dc-b4856557eed7",
   "metadata": {},
   "source": [
    "There are two ways to normalize your MC histograms to the data.\n",
    " 1. Make sure that your MC histograms are correctly normalized relative to each other, then scale them all so that their integral is the same as the integral of the data. This method is simpler to apply early as it only requires a data histogram and MC histograms without additional ingredients. However this assumes that the events in all the histograms have been rigorously selected in the exact same way. Else the integral will be identical but some features may look very different.\n",
    " 2. Implement a normalization selection (see later in the notebook) to determine a kaon flux (how many kaons entered the experiment) and scale everything relative to that number. This has the advantage that this number is valid whatever selection criterion are applied. This will allow us to better compare data and MC and clearly see features that are not conssistent between the two, without having it diluted through the whole histogram.\n",
    "    \n",
    "**Example**: To illustrate the two methods above, we are going to generate a dataset according to a complex distribution (representing data), and another dataset with a slightly different distribution (representing MC). We are then going to produce histograms according to both methods discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1094c4ee-a370-4e49-ae4e-df974d211605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define our test distributions: \n",
    "#  1. two gaussian with completely different parameters on top of a uniform distribution. The amplitude of the second gaussian can be modified from parameter.\n",
    "#  2. One simple gaussian\n",
    "# These distribution will correspond to two \"channel\" featuring different behaviour.\n",
    "from lmfit.models import GaussianModel, LinearModel\n",
    "def test_distribution1(x, amp2=100):\n",
    "    # Create the model\n",
    "    distrib_model = GaussianModel(prefix=\"g1_\") + GaussianModel(prefix=\"g2_\") + LinearModel()\n",
    "\n",
    "    # Set the parameters\n",
    "    pars = distrib_model.make_params()\n",
    "    pars[\"g1_center\"].value = 2\n",
    "    pars[\"g1_sigma\"].value = 0.5\n",
    "    pars[\"g1_amplitude\"].value = 60\n",
    "    pars[\"g2_center\"].value = 5\n",
    "    pars[\"g2_amplitude\"].value = amp2\n",
    "    pars[\"intercept\"].value = 30\n",
    "    pars[\"slope\"].value = -3\n",
    "\n",
    "    # Return the model evaluated on the provided x values\n",
    "    return distrib_model.eval(x=x, params=pars)\n",
    "\n",
    "def test_distribution2(x):\n",
    "    # Create the model\n",
    "    distrib_model = GaussianModel(prefix=\"g1_\")\n",
    "\n",
    "    # Set the parameters\n",
    "    pars = distrib_model.make_params()\n",
    "    pars[\"g1_center\"].value = 2\n",
    "    pars[\"g1_sigma\"].value = 0.5\n",
    "    pars[\"g1_amplitude\"].value = 20\n",
    "\n",
    "    # Return the model evaluated on the provided x values\n",
    "    return distrib_model.eval(x=x, params=pars)\n",
    "    \n",
    "# We will then randomly pick a dataset corresponding to our distribution. We can do this \n",
    "# using the accept-reject method\n",
    "def accept_reject(distrib, range, max, n_samples, plot_it=False):\n",
    "    # Generate x and y uniformly distributed in a 2D box\n",
    "    x = np.random.uniform(range[0], range[1], n_samples)\n",
    "    y = np.random.uniform(0, max, n_samples)\n",
    "    # Compute for each x the expected y value according to the input distribution\n",
    "    y_test = distrib(x)\n",
    "\n",
    "    # Accept only the values which are below the curve\n",
    "    accept = np.where(y<=y_test)\n",
    "    reject = np.where(y>y_test)\n",
    "\n",
    "    # We can plot to illustrate\n",
    "    if plot_it:\n",
    "        plt.scatter(x[reject], y[reject], marker=\".\", label=\"Rejected\")\n",
    "        plt.scatter(x[accept], y[accept], marker=\".\", label=\"Accepted\")\n",
    "        plt.scatter(x, y_test, marker=\"x\", label=\"Exact distribution\")\n",
    "        plt.legend(loc=\"lower left\")\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"y\")\n",
    "    return x[accept]\n",
    "\n",
    "distrib_max = 100 # For accept-reject method\n",
    "# We are going to generate data corresponding to a certain flux, and a number of MC that we fix (to have something equivalent to our physics problem with data and MC).\n",
    "# We are also going to simulate the fact that each MC sample has a different branching ratio (0.75 and 0.25)\n",
    "flux = 100000\n",
    "n_simu = 200000\n",
    "n_simu2 = 150000\n",
    "br1 = 0.75\n",
    "br2 = 0.25\n",
    "\n",
    "x = np.arange(0,10,0.1)\n",
    "ratio = 0.7528 # Don't worry about this number. It's a trick to keep the BR of the distribution1 the same after changing the amplitude of the second gaussian\n",
    "\n",
    "# Generate the data\n",
    "data = accept_reject(lambda x: test_distribution1(x) + test_distribution2(x), (0,10), distrib_max, flux, plot_it=True)\n",
    "# Generate the MC twice using the first distribution: once with the same distribution as the data, once changing the amplitude of the second Gaussian \n",
    "mc_same = accept_reject(test_distribution1, (0,10), distrib_max*br1, n_simu)\n",
    "plt.figure()\n",
    "mc = accept_reject(partial(test_distribution1, amp2=50), (0,10), distrib_max*ratio, n_simu)\n",
    "# And once for the second distribution\n",
    "mc2 = accept_reject(test_distribution2, (0,10), distrib_max*br2, n_simu2)\n",
    "# Then we do a third sample from the one with the same distribution as the data, but we remove all the events \n",
    "# with x<1 (this would correspond to an additional cut in a data selection)\n",
    "mc_same_with_cut = mc_same[mc_same>1]\n",
    "mc2_with_cut = mc2[mc2>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00408af9-e55b-4448-94e8-ffd9b4299e96",
   "metadata": {},
   "source": [
    "First please notice the nice illustration of the \"Accept-Reject\" sampling method, where we randomly generate points in a 2D space and reject all those that are above the theoretical curve that we defined.\n",
    "\n",
    "Now below let's histogram our data. Three histograms according to the first histograming method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b424fe9c-f7b4-452d-94f2-5a1855b36e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(21,5))\n",
    "weights = [0.75/n_simu, 0.25/n_simu2]\n",
    "histo.hist_data(data, bins=100, range=(0,10), ax=ax[0])\n",
    "histo.stack_mc_scale([mc_same, mc2], bins=100, range=(0,10), ndata=len(data), weights=weights, ax=ax[0])\n",
    "ax[0].set_title(\"MC generated according to same distribution as data\")\n",
    "ax[0].set_xlabel(\"x\")\n",
    "\n",
    "histo.hist_data(data, bins=100, range=(0,10), ax=ax[1])\n",
    "histo.stack_mc_scale([mc_same_with_cut, mc2_with_cut], bins=100, range=(0,10), ndata=len(data), weights=weights, ax=ax[1])\n",
    "ax[1].set_title(\"MC generated according to same distribution as data\\n, wihth cut x>1\")\n",
    "ax[1].set_xlabel(\"x\")\n",
    "\n",
    "histo.hist_data(data, bins=100, range=(0,10), ax=ax[2])\n",
    "histo.stack_mc_scale([mc, mc2], bins=100, range=(0,10), ndata=len(data), weights=weights, ax=ax[2])\n",
    "ax[2].set_title(\"MC generated according to distribution where\\n the amplitude of the second sigma is different\")\n",
    "ax[2].set_xlabel(\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0398903-511e-4ea0-b26b-bf723e98e36a",
   "metadata": {},
   "source": [
    "On these plots, we can clearly see the disadvantave of this normalization technique:\n",
    " - Everything looks fine on the first plot, which has been carefully engineered to have MC samples corresponding to the data\n",
    " - On the second plot, we added an additional cut to the MC and as a result, the entirety of the MC histograms are shifted upwards.\n",
    " - The the last plot, the largest MC contribution is actually slightly different from the data (the second gaussian is a bit smaller) and as a result the whole plot looks bad, including the first Gaussian which is actually correct in MC.\n",
    "\n",
    "On the other hand these plots have been normalized easily, with the only required input being the BR and the number of generated MC samples. Those values are generally easy to use as they are provided externally (you know the BR of your kaon decay channels, and you know how many events you simulated).\n",
    "\n",
    "Now for the second normalization technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b189c03b-412d-41ab-83f9-b3f3871d6041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `stack_mc_flux` function uses internally the `constants.kaon_br_map` for the BR of each sample. So let's introduce fake channels with the BR that we used\n",
    "# for the test distributions\n",
    "constants.kaon_br_map[\"test1\"] = 0.75\n",
    "constants.kaon_br_map[\"test2\"] = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(21,5))\n",
    "histo.hist_data(data, bins=100, range=(0,10), ax=ax[0])\n",
    "histo.stack_mc_flux({\"test1\": mc_same, \"test2\": mc2}, {\"test1\": n_simu, \"test2\": n_simu2}, bins=100, range=(0,10), kaon_flux=flux, ax=ax[0])\n",
    "ax[0].set_title(\"MC generated according to same distribution as data\")\n",
    "ax[0].set_xlabel(\"x\")\n",
    "\n",
    "histo.hist_data(data, bins=100, range=(0,10), ax=ax[1])\n",
    "histo.stack_mc_flux({\"test1\": mc_same_with_cut, \"test2\": mc2_with_cut}, {\"test1\": n_simu, \"test2\": n_simu2}, bins=100, range=(0,10), kaon_flux=flux, ax=ax[1])\n",
    "ax[1].set_title(\"MC generated according to same distribution as data\\n, wihth cut x>1\")\n",
    "ax[1].set_xlabel(\"x\")\n",
    "\n",
    "histo.hist_data(data, bins=100, range=(0,10), ax=ax[2])\n",
    "histo.stack_mc_flux({\"test1\": mc, \"test2\": mc2}, {\"test1\": n_simu, \"test2\": n_simu2}, bins=100, range=(0,10), kaon_flux=flux, ax=ax[2])\n",
    "ax[2].set_title(\"MC generated according to distribution where\\n the amplitude of the second sigma is different\")\n",
    "ax[2].set_xlabel(\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472982a4-652b-45af-898c-93b719d2aaa3",
   "metadata": {},
   "source": [
    "All the plots produced using this second method look better:\n",
    " - The first plot is obviously OK as the MC model correspond to the data model\n",
    " - The second plot also look good. The results produced by this normalization method does not change if the cuts applied to data and MC are not exaclty the same. The parts where they are will look good (x>1)\n",
    " - Aside from the second Gaussian, which is different by design, the rest of the distribution is correctly normalized. In particular the first gaussian still looks OK, as it should because its parameters are the same as in data.\n",
    "\n",
    "On the other hand we needed a bit more information to produce these plots: the number of simulated MC samples and the BR (as for the previous method), but also the kaon flux. For this example, this number was just given as input (because this is a made up example), but generally this number is unknown and depends on the data sample. A complex data seletcion must be performed to estimate it, including the estimation of the uncertainty on this number. \n",
    "\n",
    "Any serious data analysis will however have to ultimately use this second technique, while the first one can be useful to produce some quick, preliminary plots. The obtention of the kaon flux number will be the topic of one of the next section of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53020025-99a9-4cbc-a4c9-efe0d050d7e2",
   "metadata": {},
   "source": [
    "## Selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03da3a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data, but also the MC simulations we have for each channel\n",
    "# N.B. Contrary to what we were doing in previous notebooks, we are now\n",
    "#      retrieving the second output value from import_root_files.\n",
    "data, data_norm = prepare.import_root_files([\"data/run12450.root\"])\n",
    "k2pi, k2pi_norm = prepare.import_root_files([\"data/k2pi.root\"])\n",
    "k3pi, k3pi_norm = prepare.import_root_files([\"data/k3pi.root\"])\n",
    "kmu2, kmu2_norm = prepare.import_root_files([\"data/kmu2.root\"])\n",
    "kmu3, kmu3_norm = prepare.import_root_files([\"data/kmu3.root\"])\n",
    "ke3, ke3_norm = prepare.import_root_files([\"data/ke3.root\"])\n",
    "\n",
    "# And we create a dictionary with the normalization parameters that we retrieved\n",
    "normalization_dict = {\"k2pi\": k2pi_norm, \"k3pi\": k3pi_norm, \"kmu2\": kmu2_norm, \"kmu3\": kmu3_norm, \"ke3\": ke3_norm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc2e6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the weights that we will need for plotting\n",
    "total_data = len(data)\n",
    "weights = histo.compute_samples_weights(normalization_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f1e9ea-9711-4669-96ed-6e0ef6ff1199",
   "metadata": {},
   "source": [
    "### One track selection\n",
    "As you can see, we have several channels (all except K3pi) which feature only one single track in the final state. It therefore makes sense to try to have a common selection allowing us to select those channels in a similar way. Later on this has the advantage that the comparison between samples from each channel will have similar systematic errors, because the selection criterion are as similar as possible.\n",
    "\n",
    "*Note*: All examples in this section will use the first method of normalization (scaling to data) because we do not have a normalization selection yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a544ba29-dd68-45de-9982-296b5e1f5dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first create a function that will plot the invariant mass for the data and MC passed, this will save us a lot of code later\n",
    "from typing import Dict, Tuple, List, Callable\n",
    "def plot_invariant_mass(data: pd.DataFrame, mc_dict: Dict[str, pd.DataFrame], weights_dict: Dict[str, float], \n",
    "                        mass_assignment: Dict) -> Tuple[int, int]:\n",
    "    # Plot the histogram for data\n",
    "    ndata = histo.hist_data(hlf.invariant_mass(data, mass_assignment), bins=400, range=(200,600))\n",
    "\n",
    "    # Plot the histogram stack for the MC samples\n",
    "    nmc = histo.stack_mc_scale([hlf.invariant_mass(mc_dict[mc_name], mass_assignment) for mc_name in mc_dict.keys()], \n",
    "        bins=400, range=(200, 600), weights=weights_dict, labels=mc_dict.keys(), ndata=ndata)\n",
    "\n",
    "    # Some default display parameters\n",
    "    plt.legend()\n",
    "    plt.yscale(\"log\")\n",
    "    plt.ylim(bottom=0.8)\n",
    "\n",
    "    # Return the number of data events plotted and a dictionary of number of MC events plotted by sample\n",
    "    return ndata, nmc\n",
    "\n",
    "# We can also create a function that will take a list of selection criterion and apply them to the data sample and all the MC samples at the same time\n",
    "def select_all(data: pd.DataFrame, mc_dict: Dict[str, pd.DataFrame], selection_conditions: List[Callable]) -> Tuple[pd.DataFrame, Dict[str, pd.DataFrame]]:\n",
    "    data_sel = hlf.select(data, selection_conditions)\n",
    "    mc_dict_sel = {mc_name: hlf.select(mc_dict[mc_name], selection_conditions) for mc_name in mc_dict}\n",
    "    return data_sel, mc_dict_sel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fbec36-c067-412a-87e3-b0016591ec53",
   "metadata": {},
   "source": [
    "Let's start first by selecting all single-track with two-cluster events. From this we can plot the invariant mass. In the following we will work in the pion hypothesis (considering k2pi as our test case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f8b3de-0d91-4074-a486-b1a80dd63481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the k2pi assumption\n",
    "k2pi_mass_assignment = {\"track1\": constants.pion_charged_mass, \"cluster1\": constants.photon_mass, \"cluster2\": constants.photon_mass}\n",
    "plot_invariant_mass_k2pi = partial(plot_invariant_mass, mass_assignment=k2pi_mass_assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0668d39-9c33-471b-86c1-2de5303ce4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndata, nmc = plot_invariant_mass_k2pi(data, {\"k3pi\": k3pi, \"kmu2\": kmu2, \"k2pi\": k2pi, \"kmu3\": kmu3, \"ke3\": ke3}, weights)\n",
    "print(f\"K2pi selection purity: {nmc['k2pi']/sum(nmc.values()):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95155fb8-c6ed-46c2-8bbc-a901b0a1c493",
   "metadata": {},
   "source": [
    "The Data/MC agreement does not look good. The reason is that we use an inconsistent mix of various pre-selections with various efficiencies depending on the sample. We did not even select events with 1 Track and 2 clusters, which means we have really included in the plot random events.\n",
    "\n",
    "Furthermore as already mentioned, the pre-selections have a set of similar cuts, but not necessarily at the same values. We will therefore reconcile these cuts to a common value as described earlier. We define hereafter the list of cuts that can be applied to all (1-track & 2 cluster) selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc0f073-386c-4002-818e-7b29528de1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the correct topology\n",
    "cond_1T2C = hlf.make_exists_cut([\"track1\", \"cluster1\", \"cluster2\"], [])\n",
    "\n",
    "# Distance/position cuts\n",
    "lkr_dtrack_cluster_cond = partial(hlf.make_lkr_distance_cut, 150, None)\n",
    "lkr_dclusters_cond = hlf.make_lkr_distance_cut(200, None, \"cluster1\", \"cluster2\")\n",
    "z_vertex_cond = hlf.make_z_vertex_cut(120000, 180000)\n",
    "cda_cond = hlf.make_cda_cut(None, 25)\n",
    "neutral_vtx_cond = hlf.make_charged_neutral_vertex_cut(None, 10000, \"cluster1\", \"cluster2\", constants.pion_neutral_mass)\n",
    "\n",
    "# Energy/momentum cuts\n",
    "cluster_energy_cond = partial(hlf.make_energy_cut, 2000, None)\n",
    "\n",
    "# PID cuts\n",
    "rich_e_cond = partial(hlf.make_rich_cut, constants.rich_hypothesis_map[\"e\"])\n",
    "rich_pi_cond = partial(hlf.make_rich_cut, constants.rich_hypothesis_map[\"pi\"])\n",
    "rich_mu_cond = partial(hlf.make_rich_cut, constants.rich_hypothesis_map[\"mu\"])\n",
    "lkr_e_cond = partial(hlf.make_eop_cut, 0.95, 1.05)\n",
    "lkr_pi_cond = partial(hlf.make_eop_cut, None, 0.9)\n",
    "lkr_mu_cond = partial(hlf.make_eop_cut, None, 0.2)\n",
    "muv3_mu_cond = partial(hlf.make_muv3_cut, True, time_window=1.5)\n",
    "muv3_not_mu_cond = partial(hlf.make_muv3_cut, False, time_window=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dbcdd2-1594-47cd-89f1-6a6a30e067f9",
   "metadata": {},
   "source": [
    "Then we restart the plot selecting at least the correct topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29571c23-6fc6-4286-84c3-d97b10b4ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sel, mc_sel = select_all(data, {\"k3pi\": k3pi, \"kmu2\": kmu2, \"k2pi\": k2pi, \"kmu3\": kmu3, \"ke3\": ke3}, [cond_1T2C])\n",
    "ndata, nmc = plot_invariant_mass_k2pi(data_sel, mc_sel, weights)\n",
    "print(f\"K2pi selection purity: {nmc['k2pi']/sum(nmc.values()):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4eab64-3b7c-4da7-a961-5b90439f107c",
   "metadata": {},
   "source": [
    "This is already better, but we still have issues. Let's apply the remaining cuts to align the selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad16f1bb-497c-4080-ae4e-94fdcb82f079",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sel, mc_sel = select_all(data_sel, mc_sel, \n",
    "                             [z_vertex_cond, lkr_dtrack_cluster_cond(\"track1\", \"cluster1\"), lkr_dtrack_cluster_cond(\"track1\", \"cluster2\"),\n",
    "                              lkr_dclusters_cond, cluster_energy_cond(\"cluster1\"), cluster_energy_cond(\"cluster2\"), \n",
    "                              cda_cond, neutral_vtx_cond])\n",
    "ndata, nmc = plot_invariant_mass_k2pi(data_sel, mc_sel, weights)\n",
    "print(f\"K2pi selection purity: {nmc['k2pi']/sum(nmc.values()):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d154856c-8d2d-49be-9976-179d4f0b5ac7",
   "metadata": {},
   "source": [
    "These cuts have not changed the picture much. We can confirm this by looking at the acceptance of each cut (i.e. which additional fraction of events the cut will let pass)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa54fef6-89e4-4f50-a30a-ee8fd77a7140",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_sel[\"k2pi\"].attrs[\"acceptances\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2293c9-170c-4440-8ca3-bc22b5f15a7f",
   "metadata": {},
   "source": [
    "We can see that indeed each cut has less than 1% effect. The reason however is clear: these cuts have already been applied in the pre-selection, we are just applying fine tuning here (like cutting at cda<25 instead of cda<30).\n",
    "\n",
    "There are still two major differences between the selections: the PID and the reconstructed kinematic cuts. At this point however we cannot remain generic and we have to actually select specific channels. We will embrace full on the k2pi selection and move to other selections later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c692a9-684c-464e-b7a6-6a0019513a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K2pi reconstructed kinematic cuts\n",
    "k2pi_mmiss2_cond = hlf.make_missing_mass_sqr_cut(min_mm2=None, max_mm2=0.015*1e6, mass_assignments=k2pi_mass_assignment) # In GeV\n",
    "k2pi_inv_mass_cond = hlf.make_invariant_mass_cut(min_mass=460, max_mass=520, mass_assignments=k2pi_mass_assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d6e967-fe0d-4309-a325-d044812c67e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply all PID techniques\n",
    "data_sel_pid, mc_sel_pid = select_all(data_sel, mc_sel, [lkr_pi_cond(which_track=\"track1\"), muv3_not_mu_cond(which_track=\"track1\"), rich_pi_cond(which_track=\"track1\")])\n",
    "ndata, nmc = plot_invariant_mass_k2pi(data_sel_pid, mc_sel_pid, weights)\n",
    "plt.ylim(bottom=1e-2)\n",
    "print(f\"K2pi selection pollution (1-purity): {1-nmc['k2pi']/sum(nmc.values()):.2e}\")\n",
    "nmc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d8ac24-7a6c-47bc-8aaf-5d2e67d2ef0b",
   "metadata": {},
   "source": [
    "Much better, but we can still see a total of ~1.9 estimated background events mostly due to ke3 (for a pollution at the order of $10^{-6}$ which is already very good). We can further improve by adding the final kinematic cuts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afbc51c-b106-4340-b275-930f1c1be696",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sel_pid, mc_sel_pid = select_all(data_sel_pid, mc_sel_pid, [k2pi_mmiss2_cond, k2pi_inv_mass_cond])\n",
    "ndata, nmc = plot_invariant_mass_k2pi(data_sel_pid, mc_sel_pid, weights)\n",
    "plt.ylim(bottom=1e-2)\n",
    "print(f\"K2pi selection pollution (1-purity): {1-nmc['k2pi']/sum(nmc.values()):.2e}\")\n",
    "nmc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab15d2e-475f-41a0-94e6-4d2654765ad0",
   "metadata": {},
   "source": [
    "This last operation gained us another order of magnitude in purity. But what about the acceptance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1f5d2e-3e6c-479e-97ad-f5831dd2703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mc_sel_pid[\"k2pi\"].attrs[\"acceptances\"])\n",
    "print(f\"K2pi selection acceptance: {len(mc_sel_pid['k2pi'])/normalization_dict['k2pi']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7914e0-aef6-46e9-8aee-8a62abc2a658",
   "metadata": {},
   "source": [
    "We see here that the total selection acceptance is 9.08%, which means that out of all K2pi events simulated, only 9.08% are making it to the end of the selection. This seems not a lot but that is in fact the typical perforformances of a selection in NA62 if we want a pure sample. The acceptance of each individual cut that we have applied is high, and as already mentioned the reason is that most of them have already been applied in the pre-selection and their full performance is actually included in the original 15.7% acceptance of the pre-selection. \n",
    "\n",
    "What stands out is that two cuts in particular have a large impact: \n",
    " - The cut on the z vertex. In fact moving from 105 m to 120 m reduces the allowed decay volume by 20%. This translates to a 14% loss in acceptance (because of the kinematics and geometrical acceptance, the vertex Z distribution is not completely uniform - see next plot). This is a large loss, but unfortuntely a necessary one if we want to keep this selection as close as possible to the other 1-track & 2 clusters selections.\n",
    " - The RICH cut on the PID induces an enormous acceptance loss of 31.5% (relative). Let's see if if it is really necessary or if E/P and MUV3 is enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8ed697-83b2-4289-a60d-23a204e7b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Z vertex distribution of k2pi to illustrate the point made in the previous paragraph.\n",
    "k2pi[\"vtx_z\"].hist(bins=80, range=(100000, 180000))\n",
    "plt.title(\"K2pi sample vertex Z distribution\")\n",
    "plt.xlabel(\"$Z_\\mathrm{vtx}$ [mm]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07470319-495e-462b-9ff6-5c30c7fc1d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PID techniques but not RICH, directly apply also the reconstructed kinematics cuts\n",
    "data_sel_pid, mc_sel_pid = select_all(data_sel, mc_sel, [lkr_pi_cond(which_track=\"track1\"), muv3_not_mu_cond(which_track=\"track1\"), k2pi_mmiss2_cond, k2pi_inv_mass_cond])\n",
    "ndata, nmc = plot_invariant_mass_k2pi(data_sel_pid, mc_sel_pid, weights)\n",
    "plt.ylim(bottom=1e-2)\n",
    "print(f\"K2pi selection pollution (1-purity): {1-nmc['k2pi']/sum(nmc.values()):.2e}\")\n",
    "nmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49e16ee-cec0-4ffe-af4a-aaa4af058173",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mc_sel_pid[\"k2pi\"].attrs[\"acceptances\"])\n",
    "print(f\"K2pi selection acceptance: {len(mc_sel_pid['k2pi'])/normalization_dict['k2pi']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf5765c-bb11-4799-a3a7-0ede805aeb0f",
   "metadata": {},
   "source": [
    "The selection acceptance has increased by 4% absolute. But on the other hand the purity has decreased by two orders of magnitude! Depending on our objective (purity vs. acceptance) we can decide to use this extra PID condition or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e5df9c-6b3a-4acf-be52-bb7a344f8583",
   "metadata": {},
   "source": [
    "### Normalization selection\n",
    "\n",
    "Let's assume in the following that we are interested in a pure selection, because we do not actually want to measure K2pi but only use it as normalization. In NA62 this process is necessary as we do not have an absolute measurement of the kaon flux (i.e. how many kaons enter our experiment). Instead we must use a well known normalization channel (such as K2pi), measure a flux based on a normalization selection (a pure selection for K2pi) and measure our signal channel relative to the normalization channel. We can then move from a relative value to an absolute value by using the K2pi BR as an external input (with its own uncertainties, which is why we want to use a channel with existing precision measurements).\n",
    "\n",
    "As an example, the process for measuring the Ke3 branching ratio using K2pi as normalization is the following:\n",
    " - Perform the K2pi and Ke3 selections, estimate the K2pi selection acceptance $A_\\text{k2pi}$ (on K2pi events) and Ke3 selection acceptance $A_\\text{ke3}$ (on Ke3 events).\n",
    " - We know that in general the number of selected events is $N_\\text{sel} = N_K *\\text{BR} * A * \\varepsilon_\\text{trigg}$ where $N_K$ is the kaon flux, BR is the branching fraction of the channel, $A$ is the selection acceptance, and $\\varepsilon_\\text{trigg}$ is the trigger efficiency.\n",
    " - Applying this to the K2pi normalization we have: $$N_K = \\frac{N_\\text{k2pi,sel}}{A_\\text{k2pi}\\cdot\\text{BR(k2pi)}\\cdot\\varepsilon_\\text{trigg,k2pi}}$$\n",
    " - We have the equivalent equation for Ke3, into which we can substitute $N_K$ estimated from K2pi: $$N_\\text{ke3,sel} = N_K\\cdot A_\\text{ke3}\\cdot\\text{BR(ke3)}\\cdot\\varepsilon_{trigg,ke3}$$\n",
    "$$\\Rightarrow\\text{BR(ke3)} = \\frac{N_\\text{ke3,sel}}{N_K\\cdot A_\\text{ke3}\\cdot\\varepsilon_{trigg,ke3}}$$\n",
    "$$\\Rightarrow\\text{BR(ke3)} = \\frac{N_\\text{ke3,sel}}{N_\\text{k2pi,sel}}\\cdot\\frac{A_\\text{k2pi}}{A_\\text{ke3}}\\cdot\\frac{\\varepsilon_\\text{trigg,k2pi}}{\\varepsilon_{trigg,ke3}}\\cdot\\text{BR(k2pi)}$$\n",
    "This last expression gives us our final measurement as a function of the external parameter BR(k2pi). As you can see we also take ratios of equivalent quantities between Ke3 and K2pi. This means that in the assumption that this quantities are obtained and apply in a similar way for both channels (selections are closely related, trigger lines are the same and applying on similar conditions, data are acquired at the same time, ...) then they will have similar systematic uncertainties and they will cancel out in the ratio at first order.\n",
    "\n",
    "In the following we will also assume that the trigger efficiencies are 100% (real value is very high anyways, well above 90%). Please note also that the Kaon flux computed is an **effective** flux, not the absolute one. It depends on the trigger line, on the trigger downscaling, on some geometry parameters, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2a4f49-19b6-4dae-af74-5a5f41b9ffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define our complete K2pi selection\n",
    "common_1track_2cluster_selection = [cond_1T2C, z_vertex_cond, \n",
    "                                    lkr_dtrack_cluster_cond(\"track1\", \"cluster1\"), lkr_dtrack_cluster_cond(\"track1\", \"cluster2\"), lkr_dclusters_cond, \n",
    "                                    cluster_energy_cond(\"cluster1\"), cluster_energy_cond(\"cluster2\"), \n",
    "                                    cda_cond, neutral_vtx_cond]\n",
    "\n",
    "k2pi_selection = [lkr_pi_cond(which_track=\"track1\"), muv3_not_mu_cond(which_track=\"track1\"), rich_pi_cond(which_track=\"track1\"), \n",
    "                  k2pi_mmiss2_cond, k2pi_inv_mass_cond]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c62f34-3031-4ba4-8975-a34e148a989e",
   "metadata": {},
   "source": [
    "Having defined our K2pi selection for normalization, we can compute the flux $N_\\text{k2pi,sel}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462bdf99-26bc-459d-bbf2-8641b1f3e4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select first common 1-track & 2-clusters sample so we can use it multiple times without re-selecting it\n",
    "data_1T2C, mc_1T2C = select_all(data, {\"k3pi\": k3pi, \"kmu2\": kmu2, \"k2pi\": k2pi, \"kmu3\": kmu3, \"ke3\": ke3}, common_1track_2cluster_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d941ca7-2dea-4040-81ee-c7614f9c974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k2pi_mc_sel = hlf.select(mc_1T2C[\"k2pi\"], k2pi_selection)\n",
    "data_k2pi_sel = hlf.select(data_1T2C, k2pi_selection)\n",
    "k2pi_acc = len(k2pi_mc_sel)/normalization_dict[\"k2pi\"]\n",
    "k2pi_N_K = len(data_k2pi_sel)/(k2pi_acc * constants.kaon_br_map[\"k2pi\"] * 1.0)\n",
    "\n",
    "print(f\"K2pi selection acceptance: {k2pi_acc:.2%}\")\n",
    "print(f\"Number of K2pi candidates in data: {len(data_k2pi_sel)} candidates\")\n",
    "print(f\"Kaon flux: {k2pi_N_K:.2e} kaons\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
