{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6410adb-3777-494f-a9e6-c83cd9c56f70",
   "metadata": {},
   "source": [
    "# Running large statistics\n",
    "**WARNING**: This exercise requires a larger data sample than the one available with the git repository. Please download beforehand the complete **Full_DataSample** directory from the course sharepoint (including data and MC).\n",
    "\n",
    "As we have seen at the end of the previous notebook, improving the statistical uncertainty on the measurement will require running the analysis on a larger statistics. If you inspect the directory containing the full data and MC statistics, you will see that we have almost 20 GB. And this is a much reduced dataset:\n",
    "  - The data format for each event has been very simplified with respect to what is available for complete analysis\n",
    "  - A lot of data has been removed from each event, especially what concerns the pile up\n",
    "  - The samples presented to you have passed a rather strict pre-selection\n",
    "  - The data sample available to you is only about 1/5 of the full 2022 dataset (and NA62 has been collecting data since 2015)\n",
    "In total the data sample for NA62 correspond to several PB/year, so you see that 20 GB has indeed been seriously reduced. Nevertheless, if you were to try to load the complete dataset available to you in the DataFrames for analysis, you will find that this will probably not work as a pandas DataFrame is entirely loaded in the memory of your computer. You generally have only 8 GB of memory available, on some specialised computers and very powerful servers this can rise from 16 GB to 64 GB. While this may be enough for this dataset, it will certainly not be enough for the full NA62 dataset.\n",
    "\n",
    "The way to go forward is to analyze the data in chunks: we load a chunk of some reasonable size (e.g. 1 GB), we run the data through our analysis function and produce some summarized \"combinable\" results, then we go on and do the same on all the remaining chunks. Once the process is over, we have N sets of summarized \"combinable\" results. We only have to combine these results together to obtain the final one. I highlighted the \"combinable\" keyword here, by which I mean that the results can easily be merged together. Essentially:\n",
    " - Numbers that can be summed or multiplied\n",
    " - Arrays that can be summed or concatenated\n",
    "\n",
    "There are some standard packages that can perform this for us, one of them is Dask but it requires a bit of setup, configuration and infrastructure which is beyond the purpose of this exercises. It however has the nice feature to allow you to interact with your data as if you were manipulating pandas dataframes, hiding the complexities of the processing or the size of the data. Instead for this exercise which is not overly complex, we are going to run this through some custom functions that will do the job.\n",
    "\n",
    "Technically we will define all the analysis functions we need. Each function will take a parameter containing the input dataframe. The output of our function will be a structure containing a possible output dataframe (in case the function is a *selection* function, maybe a list of histograms that we prepared, maybe some numbers that were computed from the dataframe, ... We will call a function which takes our list of function as parameter as well as the lists of all the input files. The function will take care of reading this files chunk by chunk, passing the chunks to the analysis functions and retrieve the results. The results will be merged for each chunk. This may take a little while to run over all the chunks, but in the output we will recover the final merged result for each of the sample we passed (data and each MC sample). We can then combine those to produce the inal result and display the plots that were prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02299c48-305b-464b-a718-cb7de7d1b13e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
